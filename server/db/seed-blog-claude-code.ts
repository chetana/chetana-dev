import { neon } from '@neondatabase/serverless'
import { drizzle } from 'drizzle-orm/neon-http'
import { blogPosts } from './schema'
import { eq } from 'drizzle-orm'
import 'dotenv/config'
import { config } from 'dotenv'
config({ path: '.env.local' })

const sql = neon(process.env.DATABASE_URL!)
const db = drizzle(sql)

const contentFr = `## Introduction

En janvier 2026, au retour de mes vacances, j'ai dÃ©couvert Claude Code. Quelques semaines plus tard, je commenÃ§ais Ã  l'intÃ©grer dans le workflow de mon Ã©quipe de 5 personnes chez DJUST. On est encore au tout dÃ©but de l'aventure â€” c'est frais, c'est rÃ©cent, et les rÃ©sultats ne sont pas encore spectaculaires.

Ce n'est pas un article promotionnel. C'est un retour d'expÃ©rience honnÃªte â€” avec les premiers succÃ¨s, les rÃ©sistances humaines, les ajustements, et ce qu'on commence Ã  comprendre â€” sur ce que Ã§a implique concrÃ¨tement de demander Ã  une Ã©quipe d'engineering en production de changer ses habitudes pour intÃ©grer un outil IA.

---

## Chapitre 1 : Le contexte â€” une Ã©quipe sous pression

### DJUST en 2024

DJUST est une plateforme e-commerce B2B SaaS. Mon pÃ©rimÃ¨tre en tant qu'Engineering Manager couvre l'Order Management System (OMS), les Payments et le Cart. C'est le cÅ“ur transactionnel de la plateforme â€” lÃ  oÃ¹ passent les commandes de clients comme Franprix, Eiffage (Blueon) ou VEJA.

L'Ã©quipe :
- 5 personnes : 2 dÃ©veloppeurs seniors, 1 mid-level, 1 junior, et 1 QA senior
- Stack : Java 17, Spring Boot, PostgreSQL, Elasticsearch, Kubernetes sur AWS
- ~15 modules Maven interdÃ©pendants
- Releases hebdomadaires le jeudi
- SLA contractuels avec les clients enterprise

### Le problÃ¨me de productivitÃ©

En analysant oÃ¹ passait le temps de l'Ã©quipe, j'ai identifiÃ© un pattern rÃ©current :

**30% du temps Ã©tait consommÃ© par des tÃ¢ches rÃ©pÃ©titives Ã  faible valeur ajoutÃ©e :**

- **Code reviews** : 2-3 heures par jour pour moi en tant que lead technique. Chaque PR nÃ©cessitait une lecture attentive, des commentaires sur le style, la couverture de tests, les edge cases
- **Tests boilerplate** : Ã©crire des tests unitaires pour des CRUDs, des mappers, des DTOs â€” du code prÃ©visible mais chronophage
- **Briefings de dÃ©ploiement** : chaque release nÃ©cessitait un document rÃ©capitulatif des changements, des risques, des rollback plans
- **Analyse de bugs** : fouiller les logs, croiser les stacktraces avec le code, identifier le commit fautif
- **Documentation** : mettre Ã  jour les ADRs, les runbooks, les README aprÃ¨s chaque changement d'architecture

Ces tÃ¢ches ne sont pas inutiles â€” elles sont essentielles. Mais elles sont **prÃ©visibles et structurÃ©es**, ce qui les rend parfaites pour l'automatisation par IA.

## Chapitre 2 : La dÃ©couverte de Claude Code

### Le chemin avant Claude Code

Avant d'arriver Ã  Claude Code, j'ai explorÃ© d'autres outils IA :

**GitHub Copilot** (6 mois) â€” L'autocomplÃ©tion est utile, mais limitÃ©e : elle suggÃ¨re du code ligne par ligne sans comprendre le contexte global du projet. Pour du boilerplate, c'est bien. Pour de l'architecture, c'est insuffisant.

**Zencoder** â€” J'ai utilisÃ© Zencoder pour m'aider Ã  valider certaines tÃ¢ches. C'Ã©tait un bon intermÃ©diaire entre le "pas d'IA" et le "full AI-assisted". Ã‡a m'a montrÃ© le potentiel de l'IA pour les tÃ¢ches de validation et de vÃ©rification, mais l'outil restait limitÃ© dans son intÃ©gration au workflow.

**Google Gemini** â€” J'ai utilisÃ© Gemini massivement pendant plusieurs mois pour mes recherches techniques. Pour comprendre un concept, explorer une librairie, comparer des approches architecturales â€” Gemini Ã©tait mon moteur de recherche amÃ©liorÃ©. Mais il restait cantonnÃ© au navigateur, dÃ©connectÃ© du code.

### Pourquoi Claude Code a tout changÃ©

Ce qui m'a convaincu avec Claude Code :

1. **L'accÃ¨s au codebase complet** : Claude Code voit tous les fichiers du projet, comprend l'architecture, les conventions, les patterns existants
2. **Les skills personnalisÃ©s** : on peut crÃ©er des prompts rÃ©utilisables qui encapsulent le contexte mÃ©tier
3. **L'intÃ©gration MCP** : connexion native Ã  Slack, Jira, GitLab, Notion â€” Claude peut lire un ticket Jira et proposer un plan d'implÃ©mentation
4. **Le mode agentic** : Claude ne se contente pas de suggÃ©rer du code, il peut exÃ©cuter des commandes, lancer des tests, vÃ©rifier que Ã§a compile

### Le premier test : une code review automatisÃ©e

Mon premier skill Claude Code a Ã©tÃ© une code review automatisÃ©e. Le prompt :

*"Analyse cette PR GitLab. VÃ©rifie : la couverture de tests, les conventions de nommage DJUST, les edge cases manquants, les problÃ¨mes de performance potentiels, la cohÃ©rence avec l'architecture hexagonale. Produis un rapport structurÃ© avec des suggestions concrÃ¨tes."*

Le rÃ©sultat m'a bluffÃ©. Non seulement Claude identifiait des problÃ¨mes que j'aurais vus, mais il en trouvait certains que j'aurais manquÃ©s â€” notamment des race conditions subtiles dans du code asynchrone et des incohÃ©rences de nommage entre modules.

## Chapitre 3 : L'Ã©cosystÃ¨me de 25+ skills

### Architecture des skills

On a organisÃ© nos skills en 5 catÃ©gories :

### 1. Code Quality (7 skills)

- **review-pr** : analyse complÃ¨te d'une PR avec scoring
- **review-security** : audit de sÃ©curitÃ© (OWASP top 10, injection, XSS)
- **review-perf** : analyse de performance (N+1 queries, mÃ©moire, complexitÃ©)
- **check-conventions** : vÃ©rification des conventions DJUST (nommage, structure, patterns)
- **suggest-refactor** : suggestions de refactoring avec justification
- **check-api-contract** : vÃ©rification de la compatibilitÃ© backward des changements d'API
- **check-migration** : validation des migrations DB (reversibilitÃ©, performance, locks)

### 2. Testing & QA (7 skills)

C'est la catÃ©gorie qui a le plus d'impact â€” et c'est notre QA senior qui en tire le plus de valeur. Avant Claude, elle passait des heures Ã  rÃ©diger des cas de test manuellement. Maintenant, elle utilise Claude pour gÃ©nÃ©rer une premiÃ¨re matrice de tests qu'elle affine ensuite.

- **generate-unit-tests** : gÃ©nÃ©ration de tests unitaires pour une classe
- **generate-e2e-test** : gÃ©nÃ©ration de scÃ©narios E2E Ã  partir d'un ticket Jira
- **generate-test-data** : crÃ©ation de fixtures rÃ©alistes
- **analyze-test-coverage** : identification des chemins non testÃ©s
- **generate-mutation-tests** : suggestions de tests de mutation
- **generate-test-matrix** : gÃ©nÃ©ration d'une matrice de cas de test (nominal, edge cases, erreurs) Ã  partir d'une spec â€” le skill prÃ©fÃ©rÃ© de notre QA
- **explore-edge-cases** : Claude explore les combinaisons improbables qu'un humain ne penserait pas Ã  tester (valeurs limites, concurrence, encodages exotiques)

### 3. Deployment & Ops (5 skills)

- **briefing-mep** : gÃ©nÃ©ration du briefing de mise en production
- **analyze-incident** : analyse d'incident Ã  partir des logs et metrics
- **generate-rollback-plan** : plan de rollback pour une release
- **check-deploy-readiness** : checklist de dÃ©ploiement
- **post-mortem** : template de post-mortem Ã  partir d'un incident

### 4. Documentation (4 skills)

- **update-adr** : mise Ã  jour d'un Architecture Decision Record
- **generate-runbook** : crÃ©ation d'un runbook opÃ©rationnel
- **document-api** : documentation OpenAPI Ã  partir du code
- **changelog** : gÃ©nÃ©ration du changelog Ã  partir des commits

### 5. Productivity (4+ skills)

- **plan-implementation** : plan d'implÃ©mentation Ã  partir d'un ticket
- **estimate-complexity** : estimation de complexitÃ© d'un ticket
- **daily-summary** : rÃ©sumÃ© quotidien de l'activitÃ© de l'Ã©quipe
- **onboarding-guide** : guide d'onboarding contextualisÃ© pour un nouveau dÃ©veloppeur

### Le MCP : le vrai game-changer

Ce qui rend ces skills vraiment puissants, c'est l'intÃ©gration MCP (Model Context Protocol). Claude se connecte directement Ã  nos outils :

- **GitLab** : lecture des PRs, des pipelines, des commits
- **Jira** : lecture des tickets, des sprints, des epics
- **Slack** : envoi de rÃ©sumÃ©s, notifications d'incidents
- **Notion** : mise Ã  jour de la documentation

ConcrÃ¨tement, quand un dÃ©veloppeur finit une PR, il tape \`/review-pr 1234\` et Claude :
1. Lit la PR sur GitLab
2. Lit le ticket Jira associÃ©
3. Analyse le code par rapport aux conventions
4. Poste un rapport de review structurÃ©
5. Notifie sur Slack si des problÃ¨mes critiques sont trouvÃ©s

Le tout en 30 secondes au lieu de 45 minutes.

## Chapitre 4 : Les premiers rÃ©sultats (honnÃªtes)

### Ce qu'on observe aprÃ¨s quelques semaines

Soyons clairs : on n'a pas encore 3 mois de recul. On est en fÃ©vrier 2026, l'intÃ©gration a commencÃ© en janvier. Les chiffres qu'on peut donner sont des **premiÃ¨res tendances**, pas des mÃ©triques consolidÃ©es.

Ce qu'on observe concrÃ¨tement :

| TÃ¢che | Avant | Maintenant | Ressenti |
|-------|-------|------------|----------|
| Code review (premiÃ¨re passe) | 45 min | ~20 min | Gain rÃ©el, Claude prÃ©-mÃ¢che le travail |
| Tests boilerplate | 2h/feature | ~1h | Gain variable selon la complexitÃ© |
| Briefing MEP | 1h30 | ~30 min | Bon gain, le template est fiable |
| Analyse de bug | Variable | Variable | Parfois bluffant, parfois Ã  cÃ´tÃ© |

### Ce qui marche vraiment

- **Les code reviews assistÃ©es** : le gain est le plus clair. Claude dÃ©tecte des choses que la fatigue cognitive nous fait rater
- **La gÃ©nÃ©ration de tests** : pour les CRUDs et le boilerplate, c'est un vrai time-saver
- **Les briefings de MEP** : le skill produit un document structurÃ© en 30 secondes
- **La QA** : c'est la surprise. Notre QA senior utilise Claude pour gÃ©nÃ©rer des matrices de tests Ã  partir des specs Jira. Elle produit en 10 minutes ce qui prenait 2 heures. Et surtout, Claude trouve des edge cases auxquels personne n'aurait pensÃ© â€” des combinaisons de donnÃ©es exotiques, des scÃ©narios de concurrence, des cas limites sur les encodages. Elle dit que c'est "comme avoir un junior QA infatigable qui pose des questions stupides brillantes"

### Ce qui ne marche pas encore

- **L'analyse de bugs complexes** : Claude est bon sur les NPE Ã©videntes, mais sur les bugs de logique mÃ©tier, il tÃ¢tonne autant que nous
- **La vÃ©locitÃ© globale** : on ne peut pas honnÃªtement dire "+40% de productivitÃ©". C'est plus nuancÃ© â€” certaines tÃ¢ches sont 3x plus rapides, d'autres ne changent pas
- **L'adoption n'est pas uniforme** : sur 5 personnes, les 2 seniors et moi l'utilisons quotidiennement, le mid-level commence Ã  accrocher, le junior a Ã©tÃ© une vraie surprise â€” il a directement crÃ©Ã© plein de skills et voulu tout automatiser, c'est un excellent Ã©lÃ©ment qui mÃ©riterait d'aller plus haut, et la QA senior â€” ironiquement â€” est celle qui en tire le plus de valeur immÃ©diate (gÃ©nÃ©ration de cas de test, exploration de scÃ©narios edge case)

### Le vrai impact : le temps libÃ©rÃ©

Le gain le plus concret n'est pas un pourcentage. C'est que **je passe moins de temps sur les reviews mÃ©caniques et plus sur l'architecture et le mentoring**. Et Ã§a, c'est prÃ©cieux pour un Engineering Manager.

## Chapitre 5 : Les rÃ©sistances et les Ã©checs

### La rÃ©sistance humaine

Pas tout le monde Ã©tait enthousiaste au dÃ©part :

**"Ã‡a va nous remplacer"** â€” La crainte classique. J'ai dÃ» expliquer que Claude ne remplace pas les dÃ©veloppeurs, il remplace les tÃ¢ches que les dÃ©veloppeurs n'aiment pas faire. Un senior qui passe 3h par jour en code review n'est pas bien utilisÃ©. Un senior qui passe 3h par jour en conception d'architecture, si.

**"Le code gÃ©nÃ©rÃ© est mÃ©diocre"** â€” Vrai au dÃ©but. Les premiers skills produisaient du code gÃ©nÃ©rique. Il a fallu itÃ©rer sur les prompts, ajouter du contexte (conventions, exemples, patterns existants) pour obtenir un output utilisable. C'est un investissement de 2-3 semaines.

**"Je prÃ©fÃ¨re le faire moi-mÃªme"** â€” Le syndrome du "not invented here" appliquÃ© Ã  l'IA. Certains dÃ©veloppeurs ont mis du temps Ã  faire confiance aux reviews automatisÃ©es. La clÃ© : montrer que Claude trouve des bugs que les humains manquent.

### Les Ã©checs

**Skill "auto-fix-bug"** â€” On a essayÃ© de crÃ©er un skill qui fixe automatiquement les bugs Ã  partir des stacktraces. Ã‡a marchait pour les bugs simples (NPE, type mismatch) mais Ã©chouait sur les bugs logiques complexes. On l'a transformÃ© en "analyze-bug" qui propose des hypothÃ¨ses plutÃ´t que des fixes.

**Sur-confiance initiale** â€” Les premiÃ¨res semaines, certains dÃ©veloppeurs validaient les suggestions de Claude sans vÃ©rification. On a eu un incident mineur (un test E2E qui passait en CI mais cachait un faux positif). Ã‡a nous a rappelÃ© que l'IA est un outil, pas un oracle.

**CoÃ»t des tokens** â€” La facture mensuelle est significative. On a dÃ» optimiser les prompts et mettre en place des limites d'usage pour rester dans le budget.

### Le paradoxe de l'IA pour les profils en formation

C'est la question qui me travaille le plus en tant que manager. Mon junior et mon mid-level produisent plus de code, plus vite, avec moins de bugs. Sur le papier, c'est un succÃ¨s. Mais en creusant, je me demande : **est-ce qu'ils apprennent autant ?**

Quand j'Ã©tais junior, je passais des heures Ã  debugger un NPE. C'Ã©tait frustrant, mais c'est comme Ã§a que j'ai compris en profondeur le cycle de vie des objets Java. Aujourd'hui, mon junior tape un prompt et Claude lui donne la solution en 30 secondes. Il livre plus vite, mais a-t-il vraiment compris pourquoi Ã§a marchait pas ?

Mon mid-level utilise Claude pour Ã©crire des tests qu'il n'aurait jamais Ã©crits seul. Les tests sont bons. Mais est-ce qu'il a intÃ©riorisÃ© les patterns de test, ou est-ce qu'il dÃ©pend de Claude pour Ã§a ?

**Je n'ai pas la rÃ©ponse.** Ce que je fais en attendant :
- Je demande au junior de m'**expliquer** le code que Claude a gÃ©nÃ©rÃ© avant de le valider. Si tu ne peux pas l'expliquer, tu ne peux pas le committer
- J'organise des sessions de **live coding sans IA** pour garder les fondamentaux
- Je valorise la **comprÃ©hension** autant que la **livraison** dans mes Ã©valuations

C'est peut-Ãªtre la question la plus importante de cette dÃ©cennie pour les Engineering Managers : **comment former des dÃ©veloppeurs solides dans un monde oÃ¹ l'IA Ã©crit du code Ã  leur place ?** Je n'ai pas de rÃ©ponse dÃ©finitive, mais j'y rÃ©flÃ©chis chaque jour.

## Chapitre 6 : Les leÃ§ons apprises

### 1. Commencer petit, itÃ©rer vite

Ne lancez pas 25 skills d'un coup. On a commencÃ© par un seul (review-pr), on l'a peaufinÃ© pendant 2 semaines, puis on a ajoutÃ© les suivants un par un. Chaque skill nÃ©cessite du tuning spÃ©cifique au contexte de votre Ã©quipe.

### 2. Le contexte est roi

Un prompt gÃ©nÃ©rique produit un rÃ©sultat gÃ©nÃ©rique. La qualitÃ© des skills dÃ©pend directement du contexte que vous leur donnez :
- Les conventions de nommage de votre Ã©quipe
- Des exemples de code existant
- Les patterns architecturaux de votre projet
- Les erreurs frÃ©quentes Ã  surveiller

### 3. L'humain reste dans la boucle

Claude ne remplace pas la review humaine, il la prÃ©pare. Le workflow optimal : Claude fait une premiÃ¨re passe (conventions, tests, edge cases), le reviewer humain se concentre sur la logique mÃ©tier et les choix d'architecture.

### 4. Mesurer, mesurer, mesurer

Sans mÃ©triques, c'est de l'intuition. On a mis en place un dashboard simple qui track le temps passÃ© par catÃ©gorie de tÃ¢che. C'est ce qui nous a permis de prouver le ROI et de justifier le budget.

### 5. Former l'Ã©quipe au prompting

L'IA est aussi bonne que le prompt qu'on lui donne. On a organisÃ© des sessions de "prompt engineering" internes pour que chaque dÃ©veloppeur sache tirer le meilleur de Claude.

## Chapitre 7 : L'avenir â€” oÃ¹ va-t-on ?

### Ce qu'on prÃ©pare

- **Review automatique sur chaque PR** : Claude se dÃ©clenche automatiquement sur chaque merge request GitLab via un webhook
- **Tests de non-rÃ©gression intelligents** : Claude identifie quels tests doivent tourner en fonction des fichiers modifiÃ©s
- **Assistant d'architecture** : un skill qui connaÃ®t l'historique des ADR et peut suggÃ©rer des dÃ©cisions cohÃ©rentes avec le passÃ©

### Ma conviction (humble)

On est au tout dÃ©but. C'est excitant et frustrant Ã  la fois. L'IA n'est pas magique â€” elle ne transforme pas une Ã©quipe du jour au lendemain. Il faut investir du temps, convaincre, itÃ©rer, et accepter que certains collÃ¨gues ne seront pas convaincus tout de suite.

Mais je suis convaincu que les Ã©quipes qui expÃ©rimentent maintenant, mÃªme imparfaitement, auront une longueur d'avance. Pas parce que l'IA remplace les dÃ©veloppeurs, mais parce qu'elle **libÃ¨re du temps pour le travail qui compte** : penser, concevoir, mentorer.

C'est encore le dÃ©but. On verra dans 6 mois si les promesses se confirment. En attendant, on continue Ã  itÃ©rer â€” une skill Ã  la fois.

---

*Chetana YIN â€” FÃ©vrier 2026*
*Engineering Manager chez DJUST, 25+ skills Claude Code en production.*`

const contentEn = `## Introduction

In January 2026, right after coming back from vacation, I discovered Claude Code. A few weeks later, I started integrating it into my team's workflow of 5 people at DJUST. We're still at the very beginning of this journey â€” it's fresh, it's recent, and the results aren't spectacular yet.

This isn't a promotional article. It's an honest experience report â€” with early wins, human resistance, adjustments, and what we're starting to understand â€” about what it concretely means to ask an engineering team in production to change their habits and integrate an AI tool.

---

## Chapter 1: The Context â€” A Team Under Pressure

### DJUST in 2024

DJUST is a B2B SaaS e-commerce platform. My scope as Engineering Manager covers the Order Management System (OMS), Payments, and Cart. It's the transactional core of the platform â€” where orders flow for clients like Franprix, Eiffage (Blueon), and VEJA.

The team:
- 5 people: 2 senior developers, 1 mid-level, 1 junior, and 1 senior QA
- Stack: Java 17, Spring Boot, PostgreSQL, Elasticsearch, Kubernetes on AWS
- ~15 interdependent Maven modules
- Weekly Thursday releases
- Contractual SLAs with enterprise clients

### The Productivity Problem

Analyzing where the team's time went, I identified a recurring pattern:

**30% of time was consumed by low-value repetitive tasks:**

- **Code reviews**: 2-3 hours per day for me as technical lead. Each PR required careful reading, comments on style, test coverage, edge cases
- **Boilerplate tests**: writing unit tests for CRUDs, mappers, DTOs â€” predictable but time-consuming code
- **Deployment briefings**: each release needed a summary document of changes, risks, rollback plans
- **Bug analysis**: digging through logs, cross-referencing stacktraces with code, identifying the faulty commit
- **Documentation**: updating ADRs, runbooks, READMEs after every architecture change

These tasks aren't useless â€” they're essential. But they're **predictable and structured**, making them perfect for AI automation.

## Chapter 2: Discovering Claude Code

### The Road Before Claude Code

Before arriving at Claude Code, I explored other AI tools:

**GitHub Copilot** (6 months) â€” Autocompletion is useful but limited: it suggests code line by line without understanding the project's global context. For boilerplate, it's fine. For architecture, it's insufficient.

**Zencoder** â€” I used Zencoder to help validate certain tasks. It was a good intermediate between "no AI" and "full AI-assisted." It showed me AI's potential for validation and verification tasks, but the tool remained limited in its workflow integration.

**Google Gemini** â€” I used Gemini extensively for several months for my technical research. To understand a concept, explore a library, compare architectural approaches â€” Gemini was my enhanced search engine. But it stayed confined to the browser, disconnected from the code.

### Why Claude Code Changed Everything

What convinced me about Claude Code:

1. **Full codebase access**: Claude Code sees all project files, understands the architecture, conventions, and existing patterns
2. **Custom skills**: you can create reusable prompts that encapsulate business context
3. **MCP integration**: native connection to Slack, Jira, GitLab, Notion â€” Claude can read a Jira ticket and propose an implementation plan
4. **Agentic mode**: Claude doesn't just suggest code, it can execute commands, run tests, verify compilation

### The First Test: An Automated Code Review

My first Claude Code skill was an automated code review. The prompt:

*"Analyze this GitLab PR. Check: test coverage, DJUST naming conventions, missing edge cases, potential performance issues, consistency with hexagonal architecture. Produce a structured report with concrete suggestions."*

The result amazed me. Not only did Claude identify issues I would have seen, but it found some I would have missed â€” particularly subtle race conditions in asynchronous code and naming inconsistencies between modules.

## Chapter 3: The 25+ Skills Ecosystem

### Skills Architecture

We organized our skills into 5 categories:

### 1. Code Quality (7 skills)

- **review-pr**: comprehensive PR analysis with scoring
- **review-security**: security audit (OWASP top 10, injection, XSS)
- **review-perf**: performance analysis (N+1 queries, memory, complexity)
- **check-conventions**: DJUST convention verification (naming, structure, patterns)
- **suggest-refactor**: refactoring suggestions with justification
- **check-api-contract**: backward compatibility verification for API changes
- **check-migration**: DB migration validation (reversibility, performance, locks)

### 2. Testing & QA (7 skills)

This is the category with the most impact â€” and it's our senior QA who gets the most value. Before Claude, she spent hours writing test cases manually. Now she uses Claude to generate a first test matrix that she refines.

- **generate-unit-tests**: unit test generation for a class
- **generate-e2e-test**: E2E scenario generation from a Jira ticket
- **generate-test-data**: realistic fixture creation
- **analyze-test-coverage**: identification of untested paths
- **generate-mutation-tests**: mutation test suggestions
- **generate-test-matrix**: test case matrix generation (nominal, edge cases, errors) from a spec â€” our QA's favorite skill
- **explore-edge-cases**: Claude explores unlikely combinations a human wouldn't think to test (boundary values, concurrency, exotic encodings)

### 3. Deployment & Ops (5 skills)

- **briefing-mep**: production deployment briefing generation
- **analyze-incident**: incident analysis from logs and metrics
- **generate-rollback-plan**: rollback plan for a release
- **check-deploy-readiness**: deployment checklist
- **post-mortem**: post-mortem template from an incident

### 4. Documentation (4 skills)

- **update-adr**: Architecture Decision Record update
- **generate-runbook**: operational runbook creation
- **document-api**: OpenAPI documentation from code
- **changelog**: changelog generation from commits

### 5. Productivity (4+ skills)

- **plan-implementation**: implementation plan from a ticket
- **estimate-complexity**: ticket complexity estimation
- **daily-summary**: daily summary of team activity
- **onboarding-guide**: contextualized onboarding guide for new developers

### MCP: The Real Game-Changer

What makes these skills truly powerful is MCP (Model Context Protocol) integration. Claude connects directly to our tools:

- **GitLab**: reading PRs, pipelines, commits
- **Jira**: reading tickets, sprints, epics
- **Slack**: sending summaries, incident notifications
- **Notion**: updating documentation

Concretely, when a developer finishes a PR, they type \`/review-pr 1234\` and Claude:
1. Reads the PR on GitLab
2. Reads the associated Jira ticket
3. Analyzes the code against conventions
4. Posts a structured review report
5. Notifies on Slack if critical issues are found

All in 30 seconds instead of 45 minutes.

## Chapter 4: Early Results (Honest)

### What We're Seeing After a Few Weeks

Let's be clear: we don't have 3 months of data yet. It's February 2026, integration started in January. The numbers we can share are **early trends**, not consolidated metrics.

What we're concretely observing:

| Task | Before | Now | Feeling |
|------|--------|-----|---------|
| Code review (first pass) | 45 min | ~20 min | Real gain, Claude pre-chews the work |
| Boilerplate tests | 2h/feature | ~1h | Variable gain depending on complexity |
| Deployment briefing | 1h30 | ~30 min | Good gain, template is reliable |
| Bug analysis | Variable | Variable | Sometimes amazing, sometimes off |

### What's Actually Working

- **Assisted code reviews**: the clearest gain. Claude catches things that cognitive fatigue makes us miss
- **Test generation**: for CRUDs and boilerplate, it's a real time-saver
- **Deployment briefings**: the skill produces a structured document in 30 seconds
- **QA**: this is the surprise. Our senior QA uses Claude to generate test matrices from Jira specs. She produces in 10 minutes what used to take 2 hours. And above all, Claude finds edge cases nobody would have thought of â€” exotic data combinations, concurrency scenarios, encoding boundary cases. She says it's "like having a tireless junior QA who asks brilliantly stupid questions"

### What's Not Working Yet

- **Complex bug analysis**: Claude is good on obvious NPEs, but on business logic bugs, it fumbles as much as we do
- **Overall velocity**: we can't honestly claim "+40% productivity." It's more nuanced â€” some tasks are 3x faster, others don't change
- **Adoption isn't uniform**: out of 5 people, the 2 seniors and I use it daily, the mid-level is starting to get hooked, the junior was a real surprise â€” he immediately created tons of skills and wanted to automate everything, he's an excellent talent who deserves to go higher, and the senior QA â€” ironically â€” is the one getting the most immediate value (test case generation, edge case scenario exploration)

### The Real Impact: Freed Time

The most concrete gain isn't a percentage. It's that **I spend less time on mechanical reviews and more on architecture and mentoring**. And that's invaluable for an Engineering Manager.

## Chapter 5: Resistance and Failures

### Human Resistance

Not everyone was enthusiastic at first:

**"It will replace us"** â€” The classic fear. I had to explain that Claude doesn't replace developers, it replaces tasks that developers don't enjoy doing. A senior spending 3h/day on code review isn't well utilized. A senior spending 3h/day on architecture design is.

**"Generated code is mediocre"** â€” True at first. The initial skills produced generic code. We had to iterate on prompts, add context (conventions, examples, existing patterns) to get usable output. It's a 2-3 week investment.

**"I prefer doing it myself"** â€” The "not invented here" syndrome applied to AI. Some developers took time to trust automated reviews. The key: showing that Claude finds bugs that humans miss.

### Failures

**"auto-fix-bug" skill** â€” We tried creating a skill that automatically fixes bugs from stacktraces. It worked for simple bugs (NPE, type mismatch) but failed on complex logic bugs. We transformed it into "analyze-bug" that proposes hypotheses rather than fixes.

**Initial overconfidence** â€” In the first weeks, some developers validated Claude's suggestions without verification. We had a minor incident (an E2E test that passed in CI but hid a false positive). It reminded us that AI is a tool, not an oracle.

**Token costs** â€” The monthly bill is significant. We had to optimize prompts and set usage limits to stay within budget.

### The AI Paradox for Junior/Mid Developers

This is the question that weighs on me most as a manager. My junior and mid-level developers produce more code, faster, with fewer bugs. On paper, it's a success. But digging deeper, I wonder: **are they learning as much?**

When I was a junior, I spent hours debugging an NPE. It was frustrating, but that's how I deeply understood Java object lifecycle. Today, my junior types a prompt and Claude gives him the solution in 30 seconds. He ships faster, but did he really understand why it wasn't working?

My mid-level uses Claude to write tests he would never have written alone. The tests are good. But has he internalized the testing patterns, or does he depend on Claude for that?

**I don't have the answer.** What I'm doing in the meantime:
- I ask the junior to **explain** the code Claude generated before validating it. If you can't explain it, you can't commit it
- I organize **live coding sessions without AI** to keep fundamentals sharp
- I value **understanding** as much as **delivery** in my evaluations

This might be the most important question of this decade for Engineering Managers: **how do you train solid developers in a world where AI writes code for them?** I don't have a definitive answer, but I think about it every day.

## Chapter 6: Lessons Learned

### 1. Start Small, Iterate Fast

Don't launch 25 skills at once. We started with just one (review-pr), fine-tuned it for 2 weeks, then added others one by one. Each skill requires tuning specific to your team's context.

### 2. Context Is King

A generic prompt produces a generic result. Skill quality depends directly on the context you provide:
- Your team's naming conventions
- Examples of existing code
- Your project's architectural patterns
- Common errors to watch for

### 3. Keep Humans in the Loop

Claude doesn't replace human review, it prepares it. The optimal workflow: Claude does a first pass (conventions, tests, edge cases), the human reviewer focuses on business logic and architecture choices.

### 4. Measure, Measure, Measure

Without metrics, it's intuition. We set up a simple dashboard tracking time spent per task category. That's what allowed us to prove ROI and justify the budget.

### 5. Train the Team on Prompting

AI is only as good as the prompt you give it. We organized internal "prompt engineering" sessions so every developer could get the best out of Claude.

## Chapter 7: The Future â€” Where Are We Headed?

### What We're Preparing

- **Automatic review on every PR**: Claude triggers automatically on every GitLab merge request via webhook
- **Intelligent regression testing**: Claude identifies which tests should run based on modified files
- **Architecture assistant**: a skill that knows the ADR history and can suggest decisions consistent with the past

### My Conviction (Humble)

We're at the very beginning. It's exciting and frustrating at the same time. AI isn't magic â€” it doesn't transform a team overnight. You need to invest time, convince people, iterate, and accept that some colleagues won't be convinced right away.

But I'm convinced that teams experimenting now, even imperfectly, will have a head start. Not because AI replaces developers, but because it **frees time for the work that matters**: thinking, designing, mentoring.

It's still early days. We'll see in 6 months if the promises hold up. In the meantime, we keep iterating â€” one skill at a time.

---

*Chetana YIN â€” February 2026*
*Engineering Manager at DJUST, 25+ Claude Code skills in production.*`

const contentKm = `## áŸáŸá…á€áŸ’áá¸á•áŸ’áá¾á˜

á€áŸ’á“á»á„ááŸ‚á˜á€ášá¶ áŸ¢áŸ áŸ¢áŸ¦ á”á“áŸ’á‘á¶á”áŸ‹á–á¸áœá·áŸáŸ’áŸá˜á€á¶á› ááŸ’á‰á»áŸ†á”á¶á“ášá€áƒá¾á‰ Claude CodeáŸ” á‡á¶á€á¶ášá…á¶á”áŸ‹á•áŸ’áá¾á˜ááŸ’á˜á¸ ááŸ’á‰á»áŸ†á€áŸ†á–á»á„ášá½á˜á”á‰áŸ’á…á¼á›áœá¶á€áŸ’á“á»á„ workflow ášá”áŸáŸ‹á€áŸ’ášá»á˜áŸ” á™á¾á„áŸáŸ’áá·áá“áŸ…áŠáŸ†á”á¼á„á“áŸƒá€á¶ášá•áŸ’áŸá„á•áŸ’ášá¶áŸá“áŸáŸ‡áŸ”

á“áŸáŸ‡á˜á·á“á˜áŸ‚á“á‡á¶á¢ááŸ’áá”á‘á•áŸ’áŸá–áŸ’áœá•áŸ’áŸá¶á™á‘áŸáŸ” áœá¶á‡á¶ášá”á¶á™á€á¶ášááŸá”á‘á–á·áŸáŸ„á’á“áŸá–á·á â€” á‡á¶á˜á½á™á—á¶á–á‡áŸ„á‚á‡áŸá™ á€á¶ášá”ášá¶á‡áŸá™ á€á¶ášá”áŸ’ášá†á¶áŸ†á„ášá”áŸáŸ‹á˜á“á»áŸáŸ’áŸ á“á·á„á˜áŸášáŸ€á“áŠáŸ‚á›á”á¶á“ášáŸ€á“ â€” á¢áŸ†á–á¸á¢áŸ’áœá¸áŠáŸ‚á›áœá¶á˜á¶á“á“áŸá™á‡á¶á€áŸ‹áŸáŸ’ááŸ‚á„á€áŸ’á“á»á„á€á¶ášášá½á˜á”á‰áŸ’á…á¼á›á§á”á€ášááŸ AI á€áŸ’á“á»á„á€áŸ’ášá»á˜áœá·áŸáŸ’áœá€á˜áŸ’á˜áŠáŸ‚á›áŠáŸ†áá¾ášá€á¶ášá€áŸ’á“á»á„ productionáŸ”

---

## á‡áŸ†á–á¼á€á‘á¸ áŸ¡áŸ– á”ášá·á”á‘ â€” á€áŸ’ášá»á˜á€áŸ’ášáŸ„á˜áŸá˜áŸ’á–á¶á’

### DJUST á€áŸ’á“á»á„á†áŸ’á“á¶áŸ† áŸ¢áŸ áŸ¢áŸ¤

DJUST á‚áºá‡á¶áœáŸá‘á·á€á¶ e-commerce B2B SaaSáŸ” áœá·áŸá¶á›á—á¶á–ášá”áŸáŸ‹ááŸ’á‰á»áŸ†á€áŸ’á“á»á„á“á¶á˜á‡á¶ Engineering Manager á‚áŸ’ášá”áŠááŸ’áá”áŸ‹ Order Management System (OMS), Payments á“á·á„ CartáŸ” áœá¶á‡á¶áŸáŸ’á“á¼á›á”áŸ’ášáá·á”ááŸ’áá·á€á¶ášášá”áŸáŸ‹áœáŸá‘á·á€á¶ â€” á€á“áŸ’á›áŸ‚á„áŠáŸ‚á›á€á¶ášá”á‰áŸ’á‡á¶á‘á·á‰á á¼ášáŸá˜áŸ’ášá¶á”áŸ‹á¢áá·áá·á‡á“áŠá¼á…á‡á¶ Franprix, Eiffage (Blueon) á“á·á„ VEJAáŸ”

á€áŸ’ášá»á˜áŸ–
- áœá·áŸáŸ’áœá€áš áŸ¦ á“á¶á€áŸ‹ (áŸ¤ senior, áŸ¢ mid-level)
- StackáŸ– Java 17, Spring Boot, PostgreSQL, Elasticsearch, Kubernetes á›á¾ AWS
- ~áŸ¡áŸ¥ Maven modules áŠáŸ‚á›á–á¹á„á•áŸ’á¢áŸ‚á€á‚áŸ’á“á¶
- Releases ášáŸ€á„ášá¶á›áŸ‹ááŸ’á„áŸƒá–áŸ’ášá áŸáŸ’á”áá·áŸ
- SLA á€á·á…áŸ’á…áŸá“áŸ’á™á¶á‡á¶á˜á½á™á¢áá·áá·á‡á“ enterprise

### á”á‰áŸ’á á¶á•á›á·áá—á¶á–

áŠáŸ„á™áœá·á—á¶á‚á€á“áŸ’á›áŸ‚á„áŠáŸ‚á›á–áŸá›áœáŸá›á¶ášá”áŸáŸ‹á€áŸ’ášá»á˜ááŸ’ášá¼áœá”á¶á“á…áŸ†áá¶á™ ááŸ’á‰á»áŸ†á”á¶á“á€áŸ†áááŸ‹á‚áŸ†ášá¼áŠáŠáŸ‚á›áŸ—áŸ–

**áŸ£áŸ % á“áŸƒá–áŸá›áœáŸá›á¶ááŸ’ášá¼áœá”á¶á“á”áŸ’ášá¾á”áŸ’ášá¶áŸáŸ‹áŠáŸ„á™á€á·á…áŸ’á…á€á¶ášáŠáŠáŸ‚á›áŸ—áŠáŸ‚á›á˜á¶á“áá˜áŸ’á›áŸƒá‘á¶á”áŸ–**

- **Code reviews**áŸ– áŸ¢-áŸ£ á˜áŸ‰áŸ„á„á€áŸ’á“á»á„á˜á½á™ááŸ’á„áŸƒáŸá˜áŸ’ášá¶á”áŸ‹ááŸ’á‰á»áŸ†á€áŸ’á“á»á„á“á¶á˜á‡á¶ lead techniqueáŸ” PR á“á¸á˜á½á™áŸ—ááŸ’ášá¼áœá€á¶ášá€á¶ášá¢á¶á“áŠáŸ„á™á”áŸ’ášá»á„á”áŸ’ášá™áŸááŸ’á“ á˜áá·á™áŸ„á”á›áŸ‹á¢áŸ†á–á¸ style á€á¶ášá‚áŸ’ášá”áŠááŸ’áá”áŸ‹ tests edge cases
- **Tests boilerplate**áŸ– á€á¶ášáŸášáŸáŸáš unit tests áŸá˜áŸ’ášá¶á”áŸ‹ CRUDs, mappers, DTOs â€” á€á¼áŠáŠáŸ‚á›á¢á¶á…á–áŸ’á™á¶á€ášááŸá”á¶á“á”áŸ‰á»á“áŸ’ááŸ‚á…áŸ†áá¶á™á–áŸá›
- **Briefings deployment**áŸ– release á“á¸á˜á½á™áŸ—ááŸ’ášá¼áœá€á¶ášá¯á€áŸá¶ášáŸá„áŸ’ááŸá”á“áŸƒá€á¶ášá•áŸ’á›á¶áŸáŸ‹á”áŸ’áá¼áš á á¶á“á·á—áŸá™ rollback plans
- **á€á¶ášáœá·á—á¶á‚ bugs**áŸ– áŸáŸ’áœáŸ‚á„ášá€á€áŸ’á“á»á„ logs á•áŸ’á‚á¼á•áŸ’á‚á„ stacktraces á‡á¶á˜á½á™á€á¼áŠ á€áŸ†áááŸ‹ commit áŠáŸ‚á›á˜á¶á“á€áŸ†á á»áŸ
- **á¯á€áŸá¶áš**áŸ– á’áŸ’áœá¾á”á…áŸ’á…á»á”áŸ’á”á“áŸ’á“á—á¶á– ADRs, runbooks, READMEs á”á“áŸ’á‘á¶á”áŸ‹á–á¸á€á¶ášá•áŸ’á›á¶áŸáŸ‹á”áŸ’áá¼ášáŸáŸ’áá¶á”ááŸ’á™á€á˜áŸ’á˜á“á¸á˜á½á™áŸ—

á€á·á…áŸ’á…á€á¶ášá‘á¶áŸ†á„á“áŸáŸ‡á˜á·á“á˜áŸ‚á“á‚áŸ’á˜á¶á“á”áŸ’ášá™áŸ„á‡á“áŸá‘áŸ â€” áœá¶á…á¶áŸ†á”á¶á…áŸ‹áŸ” á”áŸ‰á»á“áŸ’ááŸ‚áœá¶ **á¢á¶á…á–áŸ’á™á¶á€ášááŸá”á¶á“ á“á·á„á˜á¶á“ášá…á“á¶áŸá˜áŸ’á–áŸá“áŸ’á’** áŠáŸ‚á›á’áŸ’áœá¾á±áŸ’á™áœá¶á›áŸ’á¢á¥áááŸ’á…áŸ„áŸ‡áŸá˜áŸ’ášá¶á”áŸ‹áŸáŸ’áœáŸá™á”áŸ’ášáœááŸ’áá·á€á˜áŸ’á˜áŠáŸ„á™ AIáŸ”

## á‡áŸ†á–á¼á€á‘á¸ áŸ¢áŸ– á€á¶ášášá€áƒá¾á‰ Claude Code

### á•áŸ’á›á¼áœá˜á»á“ Claude Code

á˜á»á“á–áŸá›á˜á€áŠá›áŸ‹ Claude Code ááŸ’á‰á»áŸ†á”á¶á“áŸáŸ’áœáŸ‚á„á™á›áŸ‹á§á”á€ášááŸ AI á•áŸ’áŸáŸá„á‘áŸ€ááŸ–

**GitHub Copilot** (áŸ¦ ááŸ‚) â€” Autocompletion á˜á¶á“á”áŸ’ášá™áŸ„á‡á“áŸá”áŸ‰á»á“áŸ’ááŸ‚á˜á¶á“á€á˜áŸ’ášá·ááŸ”
**Zencoder** â€” ááŸ’á‰á»áŸ†á”á¶á“á”áŸ’ášá¾ Zencoder áŠá¾á˜áŸ’á”á¸á‡á½á™á•áŸ’á‘áŸ€á„á•áŸ’á‘á¶ááŸ‹á€á·á…áŸ’á…á€á¶ášá˜á½á™á…áŸ†á“á½á“áŸ”
**Google Gemini** â€” ááŸ’á‰á»áŸ†á”á¶á“á”áŸ’ášá¾ Gemini á™áŸ‰á¶á„á…áŸ’ášá¾á“ášá™áŸˆá–áŸá›á‡á¶á…áŸ’ášá¾á“ááŸ‚áŸá˜áŸ’ášá¶á”áŸ‹á€á¶ášáŸáŸ’ášá¶áœá‡áŸ’ášá¶áœá”á…áŸ’á…áŸá€á‘áŸáŸáŸ”

### á áŸáá»á¢áŸ’áœá¸ Claude Code á”á¶á“á•áŸ’á›á¶áŸáŸ‹á”áŸ’áá¼ášá¢áŸ’áœá¸áŸ—á‘á¶áŸ†á„á¢áŸáŸ‹

á¢áŸ’áœá¸áŠáŸ‚á›á”á¶á“á”á‰áŸ’á…á»áŸ‡á”á‰áŸ’á…á¼á›ááŸ’á‰á»áŸ†á¢áŸ†á–á¸ Claude CodeáŸ–

1. **á€á¶ášá…á¼á›á”áŸ’ášá¾ codebase á–áŸá‰á›áŸá‰**áŸ– Claude Code áƒá¾á‰á¯á€áŸá¶ášá‚á˜áŸ’ášáŸ„á„á‘á¶áŸ†á„á¢áŸáŸ‹ á™á›áŸ‹áŸáŸ’áá¶á”ááŸ’á™á€á˜áŸ’á˜ conventions á“á·á„ patterns áŠáŸ‚á›á˜á¶á“áŸáŸ’ášá¶á”áŸ‹
2. **Skills á•áŸ’á‘á¶á›áŸ‹ááŸ’á›á½á“**áŸ– á¢áŸ’á“á€á¢á¶á…á”á„áŸ’á€á¾á prompts áŠáŸ‚á›á¢á¶á…á”áŸ’ášá¾á¡á¾á„áœá·á‰áŠáŸ‚á›ášá½á˜á”á‰áŸ’á…á¼á›á”ášá·á”á‘á¢á¶á‡á¸áœá€á˜áŸ’á˜
3. **á€á¶ášášá½á˜á”á‰áŸ’á…á¼á› MCP**áŸ– á€á¶ášáá—áŸ’á‡á¶á”áŸ‹áŠá¾á˜á‘áŸ… Slack, Jira, GitLab, Notion
4. **ášá”áŸ€á” agentic**áŸ– Claude á˜á·á“ááŸ’ášá¹á˜ááŸ‚áŸáŸ’á“á¾á€á¼áŠá‘áŸ áœá¶á¢á¶á…á”áŸ’ášáá·á”ááŸ’áá·á–á¶á€áŸ’á™á”á‰áŸ’á‡á¶ áŠáŸ†áá¾ášá€á¶áš tests á•áŸ’á‘áŸ€á„á•áŸ’á‘á¶ááŸ‹á€á¶áš compile

## á‡áŸ†á–á¼á€á‘á¸ áŸ£áŸ– á”áŸ’ášá–áŸá“áŸ’á’á¢áŸá€á¼á“áŸƒ 25+ Skills

á™á¾á„á”á¶á“ášáŸ€á”á…áŸ† skills ášá”áŸáŸ‹á™á¾á„á‡á¶ áŸ¥ á”áŸ’ášá—áŸá‘áŸ–

### 1. Code Quality (áŸ§ skills)
- **review-pr**áŸ– á€á¶ášáœá·á—á¶á‚ PR á–áŸá‰á›áŸá‰á‡á¶á˜á½á™á€á¶ášáŠá¶á€áŸ‹á–á·á“áŸ’á‘á»
- **review-security**áŸ– áŸáœá“á€á˜áŸ’á˜áŸá“áŸ’áá·áŸá»á (OWASP top 10)
- **review-perf**áŸ– á€á¶ášáœá·á—á¶á‚á”áŸ’ášáá·á”ááŸ’áá·á€á¶áš
- **check-conventions**áŸ– á€á¶ášá•áŸ’á‘áŸ€á„á•áŸ’á‘á¶ááŸ‹ conventions DJUST
- **suggest-refactor**áŸ– á€á¶ášáŸáŸ’á“á¾ refactoring á‡á¶á˜á½á™á€á¶ášá”á€áŸáŸ’ášá¶á™
- **check-api-contract**áŸ– á€á¶ášá•áŸ’á‘áŸ€á„á•áŸ’á‘á¶ááŸ‹á—á¶á–á†á”á‚áŸ’á“á¶áá™á€áŸ’ášáŸ„á™
- **check-migration**áŸ– á€á¶ášá•áŸ’á‘áŸ€á„á•áŸ’á‘á¶ááŸ‹ migrations DB

### 2. Testing (áŸ¥ skills)
### 3. Deployment & Ops (áŸ¥ skills)
### 4. Documentation (áŸ¤ skills)
### 5. Productivity (áŸ¤+ skills)

### MCPáŸ– Game-Changer á–á·á

á¢áŸ’áœá¸áŠáŸ‚á›á’áŸ’áœá¾á±áŸ’á™ skills á‘á¶áŸ†á„á“áŸáŸ‡á˜á¶á“áá¶á˜á–á›á–á·áá”áŸ’ášá¶á€áŠá‚áºá€á¶ášášá½á˜á”á‰áŸ’á…á¼á› MCP (Model Context Protocol)áŸ” Claude á—áŸ’á‡á¶á”áŸ‹áŠáŸ„á™á•áŸ’á‘á¶á›áŸ‹á‘áŸ…á§á”á€ášááŸášá”áŸáŸ‹á™á¾á„áŸ– GitLab, Jira, Slack, NotionáŸ”

## á‡áŸ†á–á¼á€á‘á¸ áŸ¤áŸ– á›á‘áŸ’á’á•á›áŠáŸ‚á›áœá¶áŸáŸ‹áœáŸ‚á„

| ášá„áŸ’áœá¶áŸáŸ‹ | á˜á»á“ | á€áŸ’ášáŸ„á™ | á—á¶á–áá»áŸá‚áŸ’á“á¶ |
|---------|------|-------|------------|
| á–áŸá›áœáŸá›á¶ code review á˜á’áŸ’á™á˜ | áŸ¤áŸ¥ á“á¶á‘á¸ | áŸ¡áŸ¥ á“á¶á‘á¸ | -áŸ¦áŸ§% |
| á–áŸá›áœáŸá›á¶áŸášáŸáŸáš tests | áŸ¢á˜áŸ‰áŸ„á„/feature | áŸ¤áŸ¥á“á¶á‘á¸/feature | -áŸ¦áŸ£% |
| á–áŸá›áœáŸá›á¶ briefing deployment | áŸ¡á˜áŸ‰áŸ„á„áŸ£áŸ  | áŸ¢áŸ  á“á¶á‘á¸ | -áŸ§áŸ¨% |
| Bugs ášá€áƒá¾á‰á€áŸ’á“á»á„ review | áŸ£.áŸ¢/PR | áŸ¥.áŸ¡/PR | +áŸ¥áŸ©% |
| Sprint velocity | áŸ¤áŸ¢ | áŸ¥áŸ¨ | +áŸ£áŸ¨% |

### á›áŸááŸáŸ†áá¶á“áŸ‹áŸ– +áŸ¤áŸ % á•á›á·áá—á¶á–

á›á¾ **á€á·á…áŸ’á…á€á¶ášáŠáŠáŸ‚á›áŸ—** á‡á¶á–á·áŸáŸáŸ á•á›á·áá—á¶á–á”á¶á“á€á¾á“á¡á¾á„ áŸ¤áŸ %áŸ”

## á‡áŸ†á–á¼á€á‘á¸ áŸ¥áŸ– á€á¶ášá”áŸ’ášá†á¶áŸ†á„ á“á·á„á€á¶ášá”ášá¶á‡áŸá™

**"áœá¶á“á¹á„á‡áŸ†á“á½áŸá™á¾á„"** â€” ááŸ’á‰á»áŸ†ááŸ’ášá¼áœá–á“áŸ’á™á›áŸ‹áá¶ Claude á˜á·á“á‡áŸ†á“á½áŸá¢áŸ’á“á€á¢á—á·áœáŒáŸ’áá“áŸá‘áŸ áœá¶á‡áŸ†á“á½áŸá€á·á…áŸ’á…á€á¶ášáŠáŸ‚á›á¢áŸ’á“á€á¢á—á·áœáŒáŸ’áá“áŸá˜á·á“á…á¼á›á…á·ááŸ’áá’áŸ’áœá¾áŸ”

**"á€á¼áŠáŠáŸ‚á›á”á„áŸ’á€á¾áá˜á¶á“á‚á»áá—á¶á–á‘á¶á”"** â€” á–á·áá“áŸ…áŠáŸ†á”á¼á„áŸ” á™á¾á„ááŸ’ášá¼áœá’áŸ’áœá¾á¡á¾á„áœá·á‰á›á¾ prompts á”á“áŸ’ááŸ‚á˜á”ášá·á”á‘ áŠá¾á˜áŸ’á”á¸á‘á‘á½á›á”á¶á“ output áŠáŸ‚á›á¢á¶á…á”áŸ’ášá¾á”á¶á“áŸ”

## á‡áŸ†á–á¼á€á‘á¸ áŸ¦áŸ– á˜áŸášáŸ€á“áŠáŸ‚á›á”á¶á“ášáŸ€á“

1. **á…á¶á”áŸ‹á•áŸ’áá¾á˜áá¼á… á’áŸ’áœá¾á¡á¾á„áœá·á‰á›á¿á“** â€” á€á»áŸ†á…á¶á”áŸ‹á•áŸ’áá¾á˜ 25 skills á€áŸ’á“á»á„á–áŸá›ááŸ‚á˜á½á™
2. **á”ášá·á”á‘á‡á¶áŸáŸ’ááŸá…** â€” Prompt á‘á¼á‘áŸ…á•á›á·áá›á‘áŸ’á’á•á›á‘á¼á‘áŸ…
3. **ášá€áŸ’áŸá¶á˜á“á»áŸáŸ’áŸá€áŸ’á“á»á„ášá„áŸ’áœá„áŸ‹** â€” Claude ášáŸ€á”á…áŸ† review á˜á“á»áŸáŸ’áŸá•áŸ’ááŸ„áá›á¾áá€áŸ’á€áœá·á‡áŸ’á‡á¶á¢á¶á‡á¸áœá€á˜áŸ’á˜
4. **áœá¶áŸáŸ‹ áœá¶áŸáŸ‹ áœá¶áŸáŸ‹** â€” á‚áŸ’á˜á¶á“ metrics áœá¶á‡á¶á€á¶ášáŸáŸ’á˜á¶á“
5. **á”ááŸ’áá»áŸ‡á”ááŸ’áá¶á›á€áŸ’ášá»á˜á›á¾ prompting** â€” AI á›áŸ’á¢á”áŸ‰á»ááŸ’áá¶á€áŸáŠáŸ„á™ prompt áŠáŸ‚á›á¢áŸ’á“á€á•áŸ’áá›áŸ‹

## á‡áŸ†á–á¼á€á‘á¸ áŸ§áŸ– á¢á“á¶á‚á

á™á¾á„áŸáŸ’áá·áá“áŸ…áŠá¾á˜áŠáŸ†á”á¼á„á“áŸƒá”áŠá·áœááŸ’áá“áŸ AI á€áŸ’á“á»á„áœá·áŸáŸ’áœá€á˜áŸ’á˜áŸ” á€áŸ’á“á»á„ášá™áŸˆá–áŸá› áŸ¢ á†áŸ’á“á¶áŸ† á€á¶ášá˜á·á“á”áŸ’ášá¾ AI á€áŸ’á“á»á„ workflow á¢á—á·áœáŒáŸ’áá“áŸášá”áŸáŸ‹á¢áŸ’á“á€á“á¹á„á á½org áŸá˜áŸá™áŠá¼á…á‡á¶á€á¶ášá˜á·á“á”áŸ’ášá¾ linteráŸ”

AI á˜á·á“á˜áŸ‚á“á‡á¶ášá¿á„á›áŸá„á‘áŸáŸ” áœá¶á‡á¶ **á€á˜áŸ’á›á¶áŸ†á„á–á„áŸ’ášá¸á€á‡á¶á€áŸ‹áŸáŸ’ááŸ‚á„**áŸ” á á¾á™á–áŸá›áœáŸá›á¶á›áŸ’á¢á”áŸ†á•á»ááŠá¾á˜áŸ’á”á¸ášá½á˜á”á‰áŸ’á…á¼á›áœá¶á‚áºá¥á¡á¼áœá“áŸáŸ‡áŸ”

---

*Chetana YIN â€” á€á»á˜áŸ’á—áŸˆ áŸ¢áŸ áŸ¢áŸ¦*
*Engineering Manager á“áŸ… DJUST, 25+ Claude Code skills á€áŸ’á“á»á„ productionáŸ”*`

async function seedBlogClaudeCode() {
  console.log('ğŸ¤– Seeding blog article: Claude Code in Engineering Team...')

  await db.delete(blogPosts).where(eq(blogPosts.slug, 'claude-code-equipe-engineering'))

  await db.insert(blogPosts).values({
    slug: 'claude-code-equipe-engineering',
    titleFr: "Comment j'ai intÃ©grÃ© Claude Code dans mon Ã©quipe d'engineering",
    titleEn: "How I integrated Claude Code into my engineering team",
    titleKm: "ášá”áŸ€á”áŠáŸ‚á›ááŸ’á‰á»áŸ†á”á¶á“ášá½á˜á”á‰áŸ’á…á¼á› Claude Code á€áŸ’á“á»á„á€áŸ’ášá»á˜áœá·áŸáŸ’áœá€á˜áŸ’á˜ášá”áŸáŸ‹ááŸ’á‰á»áŸ†",
    contentFr,
    contentEn,
    contentKm,
    excerptFr: "Retour d'expÃ©rience honnÃªte sur l'intÃ©gration de Claude Code dans une Ã©quipe de 5 personnes : premiers rÃ©sultats aprÃ¨s quelques semaines, rÃ©sistances humaines, et ce qu'on commence Ã  comprendre.",
    excerptEn: "Honest experience report on integrating Claude Code into a team of 5: early results after a few weeks, human resistance, and what we're starting to understand.",
    excerptKm: "ášá”á¶á™á€á¶ášááŸá”á‘á–á·áŸáŸ„á’á“áŸá›á¾á€á¶ášášá½á˜á”á‰áŸ’á…á¼á› Claude Code á€áŸ’á“á»á„á€áŸ’ášá»á˜áœá·áŸáŸ’áœá€áš áŸ¦ á“á¶á€áŸ‹áŸ– 25+ skills á•áŸ’á‘á¶á›áŸ‹ááŸ’á›á½á“ +áŸ¤áŸ % á•á›á·áá—á¶á– á€á¶ášá”áŸ’ášá†á¶áŸ†á„ á“á·á„á˜áŸášáŸ€á“áŠáŸ‚á›á”á¶á“ášáŸ€á“áŸ”",
    tags: ['AI', 'Claude Code', 'Management', 'Productivity', 'MCP'],
    published: true
  })

  console.log('âœ… Blog article seeded successfully!')
}

seedBlogClaudeCode().catch(console.error)
