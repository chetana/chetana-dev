import { neon } from '@neondatabase/serverless'
import { drizzle } from 'drizzle-orm/neon-http'
import { blogPosts } from './schema'
import { eq } from 'drizzle-orm'
import 'dotenv/config'
import { config } from 'dotenv'
config({ path: '.env.local' })

const sql = neon(process.env.DATABASE_URL!)
const db = drizzle(sql)

const contentFr = `## Introduction

En novembre 2024, j'ai dÃ©couvert Claude Code. Deux semaines plus tard, je l'avais intÃ©grÃ© dans le workflow quotidien de mon Ã©quipe de 6 ingÃ©nieurs chez DJUST. Trois mois aprÃ¨s, notre productivitÃ© sur les tÃ¢ches rÃ©pÃ©titives avait augmentÃ© de 40%.

Ce n'est pas un article promotionnel. C'est un retour d'expÃ©rience brut â€” avec les succÃ¨s, les Ã©checs, les rÃ©sistances humaines, et les leÃ§ons apprises â€” sur ce que Ã§a implique concrÃ¨tement d'intÃ©grer un outil IA dans une Ã©quipe d'engineering qui tourne en production.

---

## Chapitre 1 : Le contexte â€” une Ã©quipe sous pression

### DJUST en 2024

DJUST est une plateforme e-commerce B2B SaaS. Mon pÃ©rimÃ¨tre en tant qu'Engineering Manager couvre l'Order Management System (OMS), les Payments et le Cart. C'est le cÅ“ur transactionnel de la plateforme â€” lÃ  oÃ¹ passent les commandes de clients comme Franprix, Eiffage (Blueon) ou VEJA.

L'Ã©quipe :
- 6 ingÃ©nieurs (4 seniors, 2 mid-level)
- Stack : Java 17, Spring Boot, PostgreSQL, Elasticsearch, Kubernetes sur AWS
- ~15 modules Maven interdÃ©pendants
- Releases hebdomadaires le jeudi
- SLA contractuels avec les clients enterprise

### Le problÃ¨me de productivitÃ©

En analysant oÃ¹ passait le temps de l'Ã©quipe, j'ai identifiÃ© un pattern rÃ©current :

**30% du temps Ã©tait consommÃ© par des tÃ¢ches rÃ©pÃ©titives Ã  faible valeur ajoutÃ©e :**

- **Code reviews** : 2-3 heures par jour pour moi en tant que lead technique. Chaque PR nÃ©cessitait une lecture attentive, des commentaires sur le style, la couverture de tests, les edge cases
- **Tests boilerplate** : Ã©crire des tests unitaires pour des CRUDs, des mappers, des DTOs â€” du code prÃ©visible mais chronophage
- **Briefings de dÃ©ploiement** : chaque release nÃ©cessitait un document rÃ©capitulatif des changements, des risques, des rollback plans
- **Analyse de bugs** : fouiller les logs, croiser les stacktraces avec le code, identifier le commit fautif
- **Documentation** : mettre Ã  jour les ADRs, les runbooks, les README aprÃ¨s chaque changement d'architecture

Ces tÃ¢ches ne sont pas inutiles â€” elles sont essentielles. Mais elles sont **prÃ©visibles et structurÃ©es**, ce qui les rend parfaites pour l'automatisation par IA.

## Chapitre 2 : La dÃ©couverte de Claude Code

### Le chemin avant Claude Code

Avant d'arriver Ã  Claude Code, j'ai explorÃ© d'autres outils IA :

**GitHub Copilot** (6 mois) â€” L'autocomplÃ©tion est utile, mais limitÃ©e : elle suggÃ¨re du code ligne par ligne sans comprendre le contexte global du projet. Pour du boilerplate, c'est bien. Pour de l'architecture, c'est insuffisant.

**Zencoder** â€” J'ai utilisÃ© Zencoder pour m'aider Ã  valider certaines tÃ¢ches. C'Ã©tait un bon intermÃ©diaire entre le "pas d'IA" et le "full AI-assisted". Ã‡a m'a montrÃ© le potentiel de l'IA pour les tÃ¢ches de validation et de vÃ©rification, mais l'outil restait limitÃ© dans son intÃ©gration au workflow.

**Google Gemini** â€” J'ai utilisÃ© Gemini massivement pendant plusieurs mois pour mes recherches techniques. Pour comprendre un concept, explorer une librairie, comparer des approches architecturales â€” Gemini Ã©tait mon moteur de recherche amÃ©liorÃ©. Mais il restait cantonnÃ© au navigateur, dÃ©connectÃ© du code.

### Pourquoi Claude Code a tout changÃ©

Ce qui m'a convaincu avec Claude Code :

1. **L'accÃ¨s au codebase complet** : Claude Code voit tous les fichiers du projet, comprend l'architecture, les conventions, les patterns existants
2. **Les skills personnalisÃ©s** : on peut crÃ©er des prompts rÃ©utilisables qui encapsulent le contexte mÃ©tier
3. **L'intÃ©gration MCP** : connexion native Ã  Slack, Jira, GitLab, Notion â€” Claude peut lire un ticket Jira et proposer un plan d'implÃ©mentation
4. **Le mode agentic** : Claude ne se contente pas de suggÃ©rer du code, il peut exÃ©cuter des commandes, lancer des tests, vÃ©rifier que Ã§a compile

### Le premier test : une code review automatisÃ©e

Mon premier skill Claude Code a Ã©tÃ© une code review automatisÃ©e. Le prompt :

*"Analyse cette PR GitLab. VÃ©rifie : la couverture de tests, les conventions de nommage DJUST, les edge cases manquants, les problÃ¨mes de performance potentiels, la cohÃ©rence avec l'architecture hexagonale. Produis un rapport structurÃ© avec des suggestions concrÃ¨tes."*

Le rÃ©sultat m'a bluffÃ©. Non seulement Claude identifiait des problÃ¨mes que j'aurais vus, mais il en trouvait certains que j'aurais manquÃ©s â€” notamment des race conditions subtiles dans du code asynchrone et des incohÃ©rences de nommage entre modules.

## Chapitre 3 : L'Ã©cosystÃ¨me de 25+ skills

### Architecture des skills

On a organisÃ© nos skills en 5 catÃ©gories :

### 1. Code Quality (7 skills)

- **review-pr** : analyse complÃ¨te d'une PR avec scoring
- **review-security** : audit de sÃ©curitÃ© (OWASP top 10, injection, XSS)
- **review-perf** : analyse de performance (N+1 queries, mÃ©moire, complexitÃ©)
- **check-conventions** : vÃ©rification des conventions DJUST (nommage, structure, patterns)
- **suggest-refactor** : suggestions de refactoring avec justification
- **check-api-contract** : vÃ©rification de la compatibilitÃ© backward des changements d'API
- **check-migration** : validation des migrations DB (reversibilitÃ©, performance, locks)

### 2. Testing (5 skills)

- **generate-unit-tests** : gÃ©nÃ©ration de tests unitaires pour une classe
- **generate-e2e-test** : gÃ©nÃ©ration de scÃ©narios E2E Ã  partir d'un ticket Jira
- **generate-test-data** : crÃ©ation de fixtures rÃ©alistes
- **analyze-test-coverage** : identification des chemins non testÃ©s
- **generate-mutation-tests** : suggestions de tests de mutation

### 3. Deployment & Ops (5 skills)

- **briefing-mep** : gÃ©nÃ©ration du briefing de mise en production
- **analyze-incident** : analyse d'incident Ã  partir des logs et metrics
- **generate-rollback-plan** : plan de rollback pour une release
- **check-deploy-readiness** : checklist de dÃ©ploiement
- **post-mortem** : template de post-mortem Ã  partir d'un incident

### 4. Documentation (4 skills)

- **update-adr** : mise Ã  jour d'un Architecture Decision Record
- **generate-runbook** : crÃ©ation d'un runbook opÃ©rationnel
- **document-api** : documentation OpenAPI Ã  partir du code
- **changelog** : gÃ©nÃ©ration du changelog Ã  partir des commits

### 5. Productivity (4+ skills)

- **plan-implementation** : plan d'implÃ©mentation Ã  partir d'un ticket
- **estimate-complexity** : estimation de complexitÃ© d'un ticket
- **daily-summary** : rÃ©sumÃ© quotidien de l'activitÃ© de l'Ã©quipe
- **onboarding-guide** : guide d'onboarding contextualisÃ© pour un nouveau dÃ©veloppeur

### Le MCP : le vrai game-changer

Ce qui rend ces skills vraiment puissants, c'est l'intÃ©gration MCP (Model Context Protocol). Claude se connecte directement Ã  nos outils :

- **GitLab** : lecture des PRs, des pipelines, des commits
- **Jira** : lecture des tickets, des sprints, des epics
- **Slack** : envoi de rÃ©sumÃ©s, notifications d'incidents
- **Notion** : mise Ã  jour de la documentation

ConcrÃ¨tement, quand un dÃ©veloppeur finit une PR, il tape \`/review-pr 1234\` et Claude :
1. Lit la PR sur GitLab
2. Lit le ticket Jira associÃ©
3. Analyse le code par rapport aux conventions
4. Poste un rapport de review structurÃ©
5. Notifie sur Slack si des problÃ¨mes critiques sont trouvÃ©s

Le tout en 30 secondes au lieu de 45 minutes.

## Chapitre 4 : Les rÃ©sultats mesurÃ©s

### MÃ©thodologie

On a mesurÃ© la productivitÃ© sur 3 mois (dÃ©cembre 2024 - fÃ©vrier 2025) en comparant avec les 3 mois prÃ©cÃ©dents :

| MÃ©trique | Avant | AprÃ¨s | Delta |
|----------|-------|-------|-------|
| Temps moyen de code review | 45 min | 15 min | -67% |
| Temps d'Ã©criture de tests | 2h/feature | 45 min/feature | -63% |
| Temps de briefing MEP | 1h30 | 20 min | -78% |
| Bugs dÃ©tectÃ©s en review | 3.2/PR | 5.1/PR | +59% |
| Temps d'analyse d'incident | 2h | 40 min | -67% |
| Velocity sprint (story points) | 42 | 58 | +38% |

### Le chiffre clÃ© : +40% de productivitÃ©

Sur les **tÃ¢ches rÃ©pÃ©titives** spÃ©cifiquement, la productivitÃ© a augmentÃ© de 40%. Ce chiffre exclut les tÃ¢ches crÃ©atives (conception, architecture, discussions produit) oÃ¹ l'IA n'a pas d'impact mesurable.

### Ce que les chiffres ne montrent pas

- **La qualitÃ© des reviews a augmentÃ©** : Claude trouve des edge cases que les humains manquent par fatigue cognitive
- **Le moral de l'Ã©quipe a montÃ©** : personne n'aime Ã©crire des tests boilerplate. Automatiser Ã§a, c'est libÃ©rer du temps pour le travail intÃ©ressant
- **L'onboarding est plus rapide** : un nouveau dÃ©veloppeur peut utiliser les skills dÃ¨s le jour 1 pour comprendre le codebase

## Chapitre 5 : Les rÃ©sistances et les Ã©checs

### La rÃ©sistance humaine

Pas tout le monde Ã©tait enthousiaste au dÃ©part :

**"Ã‡a va nous remplacer"** â€” La crainte classique. J'ai dÃ» expliquer que Claude ne remplace pas les dÃ©veloppeurs, il remplace les tÃ¢ches que les dÃ©veloppeurs n'aiment pas faire. Un senior qui passe 3h par jour en code review n'est pas bien utilisÃ©. Un senior qui passe 3h par jour en conception d'architecture, si.

**"Le code gÃ©nÃ©rÃ© est mÃ©diocre"** â€” Vrai au dÃ©but. Les premiers skills produisaient du code gÃ©nÃ©rique. Il a fallu itÃ©rer sur les prompts, ajouter du contexte (conventions, exemples, patterns existants) pour obtenir un output utilisable. C'est un investissement de 2-3 semaines.

**"Je prÃ©fÃ¨re le faire moi-mÃªme"** â€” Le syndrome du "not invented here" appliquÃ© Ã  l'IA. Certains dÃ©veloppeurs ont mis du temps Ã  faire confiance aux reviews automatisÃ©es. La clÃ© : montrer que Claude trouve des bugs que les humains manquent.

### Les Ã©checs

**Skill "auto-fix-bug"** â€” On a essayÃ© de crÃ©er un skill qui fixe automatiquement les bugs Ã  partir des stacktraces. Ã‡a marchait pour les bugs simples (NPE, type mismatch) mais Ã©chouait sur les bugs logiques complexes. On l'a transformÃ© en "analyze-bug" qui propose des hypothÃ¨ses plutÃ´t que des fixes.

**Sur-confiance initiale** â€” Les premiÃ¨res semaines, certains dÃ©veloppeurs validaient les suggestions de Claude sans vÃ©rification. On a eu un incident mineur (un test E2E qui passait en CI mais cachait un faux positif). Ã‡a nous a rappelÃ© que l'IA est un outil, pas un oracle.

**CoÃ»t des tokens** â€” Ã€ raison de 6 dÃ©veloppeurs qui utilisent Claude intensivement, la facture mensuelle est significative. On a dÃ» optimiser les prompts et mettre en place des limites d'usage pour rester dans le budget.

## Chapitre 6 : Les leÃ§ons apprises

### 1. Commencer petit, itÃ©rer vite

Ne lancez pas 25 skills d'un coup. On a commencÃ© par un seul (review-pr), on l'a peaufinÃ© pendant 2 semaines, puis on a ajoutÃ© les suivants un par un. Chaque skill nÃ©cessite du tuning spÃ©cifique au contexte de votre Ã©quipe.

### 2. Le contexte est roi

Un prompt gÃ©nÃ©rique produit un rÃ©sultat gÃ©nÃ©rique. La qualitÃ© des skills dÃ©pend directement du contexte que vous leur donnez :
- Les conventions de nommage de votre Ã©quipe
- Des exemples de code existant
- Les patterns architecturaux de votre projet
- Les erreurs frÃ©quentes Ã  surveiller

### 3. L'humain reste dans la boucle

Claude ne remplace pas la review humaine, il la prÃ©pare. Le workflow optimal : Claude fait une premiÃ¨re passe (conventions, tests, edge cases), le reviewer humain se concentre sur la logique mÃ©tier et les choix d'architecture.

### 4. Mesurer, mesurer, mesurer

Sans mÃ©triques, c'est de l'intuition. On a mis en place un dashboard simple qui track le temps passÃ© par catÃ©gorie de tÃ¢che. C'est ce qui nous a permis de prouver le ROI et de justifier le budget.

### 5. Former l'Ã©quipe au prompting

L'IA est aussi bonne que le prompt qu'on lui donne. On a organisÃ© des sessions de "prompt engineering" internes pour que chaque dÃ©veloppeur sache tirer le meilleur de Claude.

## Chapitre 7 : L'avenir â€” oÃ¹ va-t-on ?

### Ce qu'on prÃ©pare

- **Review automatique sur chaque PR** : Claude se dÃ©clenche automatiquement sur chaque merge request GitLab via un webhook
- **Tests de non-rÃ©gression intelligents** : Claude identifie quels tests doivent tourner en fonction des fichiers modifiÃ©s
- **Assistant d'architecture** : un skill qui connaÃ®t l'historique des ADR et peut suggÃ©rer des dÃ©cisions cohÃ©rentes avec le passÃ©

### Ma conviction

On est au tout dÃ©but de la rÃ©volution IA dans l'engineering. Dans 2 ans, ne pas utiliser d'IA dans son workflow de dÃ©veloppement sera aussi anachronique que ne pas utiliser de linter.

Les Ã©quipes qui adoptent ces outils maintenant auront un avantage compÃ©titif massif â€” pas parce que l'IA est magique, mais parce qu'elle libÃ¨re les humains pour faire ce qu'ils font de mieux : penser, concevoir, innover.

L'IA n'est pas un gadget. C'est un **multiplicateur de force concret**. Et le meilleur moment pour l'intÃ©grer, c'est maintenant.

---

*Chetana YIN â€” FÃ©vrier 2026*
*Engineering Manager chez DJUST, 25+ skills Claude Code en production.*`

const contentEn = `## Introduction

In November 2024, I discovered Claude Code. Two weeks later, I had integrated it into my team's daily workflow of 6 engineers at DJUST. Three months later, our productivity on repetitive tasks had increased by 40%.

This isn't a promotional article. It's a raw experience report â€” with successes, failures, human resistance, and lessons learned â€” about what it concretely means to integrate an AI tool into an engineering team running in production.

---

## Chapter 1: The Context â€” A Team Under Pressure

### DJUST in 2024

DJUST is a B2B SaaS e-commerce platform. My scope as Engineering Manager covers the Order Management System (OMS), Payments, and Cart. It's the transactional core of the platform â€” where orders flow for clients like Franprix, Eiffage (Blueon), and VEJA.

The team:
- 6 engineers (4 seniors, 2 mid-level)
- Stack: Java 17, Spring Boot, PostgreSQL, Elasticsearch, Kubernetes on AWS
- ~15 interdependent Maven modules
- Weekly Thursday releases
- Contractual SLAs with enterprise clients

### The Productivity Problem

Analyzing where the team's time went, I identified a recurring pattern:

**30% of time was consumed by low-value repetitive tasks:**

- **Code reviews**: 2-3 hours per day for me as technical lead. Each PR required careful reading, comments on style, test coverage, edge cases
- **Boilerplate tests**: writing unit tests for CRUDs, mappers, DTOs â€” predictable but time-consuming code
- **Deployment briefings**: each release needed a summary document of changes, risks, rollback plans
- **Bug analysis**: digging through logs, cross-referencing stacktraces with code, identifying the faulty commit
- **Documentation**: updating ADRs, runbooks, READMEs after every architecture change

These tasks aren't useless â€” they're essential. But they're **predictable and structured**, making them perfect for AI automation.

## Chapter 2: Discovering Claude Code

### The Road Before Claude Code

Before arriving at Claude Code, I explored other AI tools:

**GitHub Copilot** (6 months) â€” Autocompletion is useful but limited: it suggests code line by line without understanding the project's global context. For boilerplate, it's fine. For architecture, it's insufficient.

**Zencoder** â€” I used Zencoder to help validate certain tasks. It was a good intermediate between "no AI" and "full AI-assisted." It showed me AI's potential for validation and verification tasks, but the tool remained limited in its workflow integration.

**Google Gemini** â€” I used Gemini extensively for several months for my technical research. To understand a concept, explore a library, compare architectural approaches â€” Gemini was my enhanced search engine. But it stayed confined to the browser, disconnected from the code.

### Why Claude Code Changed Everything

What convinced me about Claude Code:

1. **Full codebase access**: Claude Code sees all project files, understands the architecture, conventions, and existing patterns
2. **Custom skills**: you can create reusable prompts that encapsulate business context
3. **MCP integration**: native connection to Slack, Jira, GitLab, Notion â€” Claude can read a Jira ticket and propose an implementation plan
4. **Agentic mode**: Claude doesn't just suggest code, it can execute commands, run tests, verify compilation

### The First Test: An Automated Code Review

My first Claude Code skill was an automated code review. The prompt:

*"Analyze this GitLab PR. Check: test coverage, DJUST naming conventions, missing edge cases, potential performance issues, consistency with hexagonal architecture. Produce a structured report with concrete suggestions."*

The result amazed me. Not only did Claude identify issues I would have seen, but it found some I would have missed â€” particularly subtle race conditions in asynchronous code and naming inconsistencies between modules.

## Chapter 3: The 25+ Skills Ecosystem

### Skills Architecture

We organized our skills into 5 categories:

### 1. Code Quality (7 skills)

- **review-pr**: comprehensive PR analysis with scoring
- **review-security**: security audit (OWASP top 10, injection, XSS)
- **review-perf**: performance analysis (N+1 queries, memory, complexity)
- **check-conventions**: DJUST convention verification (naming, structure, patterns)
- **suggest-refactor**: refactoring suggestions with justification
- **check-api-contract**: backward compatibility verification for API changes
- **check-migration**: DB migration validation (reversibility, performance, locks)

### 2. Testing (5 skills)

- **generate-unit-tests**: unit test generation for a class
- **generate-e2e-test**: E2E scenario generation from a Jira ticket
- **generate-test-data**: realistic fixture creation
- **analyze-test-coverage**: identification of untested paths
- **generate-mutation-tests**: mutation test suggestions

### 3. Deployment & Ops (5 skills)

- **briefing-mep**: production deployment briefing generation
- **analyze-incident**: incident analysis from logs and metrics
- **generate-rollback-plan**: rollback plan for a release
- **check-deploy-readiness**: deployment checklist
- **post-mortem**: post-mortem template from an incident

### 4. Documentation (4 skills)

- **update-adr**: Architecture Decision Record update
- **generate-runbook**: operational runbook creation
- **document-api**: OpenAPI documentation from code
- **changelog**: changelog generation from commits

### 5. Productivity (4+ skills)

- **plan-implementation**: implementation plan from a ticket
- **estimate-complexity**: ticket complexity estimation
- **daily-summary**: daily summary of team activity
- **onboarding-guide**: contextualized onboarding guide for new developers

### MCP: The Real Game-Changer

What makes these skills truly powerful is MCP (Model Context Protocol) integration. Claude connects directly to our tools:

- **GitLab**: reading PRs, pipelines, commits
- **Jira**: reading tickets, sprints, epics
- **Slack**: sending summaries, incident notifications
- **Notion**: updating documentation

Concretely, when a developer finishes a PR, they type \`/review-pr 1234\` and Claude:
1. Reads the PR on GitLab
2. Reads the associated Jira ticket
3. Analyzes the code against conventions
4. Posts a structured review report
5. Notifies on Slack if critical issues are found

All in 30 seconds instead of 45 minutes.

## Chapter 4: Measured Results

### Methodology

We measured productivity over 3 months (December 2024 - February 2025) comparing with the previous 3 months:

| Metric | Before | After | Delta |
|--------|--------|-------|-------|
| Average code review time | 45 min | 15 min | -67% |
| Test writing time | 2h/feature | 45 min/feature | -63% |
| Deployment briefing time | 1h30 | 20 min | -78% |
| Bugs detected in review | 3.2/PR | 5.1/PR | +59% |
| Incident analysis time | 2h | 40 min | -67% |
| Sprint velocity (story points) | 42 | 58 | +38% |

### The Key Number: +40% Productivity

On **repetitive tasks** specifically, productivity increased by 40%. This number excludes creative tasks (design, architecture, product discussions) where AI has no measurable impact.

### What the Numbers Don't Show

- **Review quality increased**: Claude finds edge cases that humans miss due to cognitive fatigue
- **Team morale improved**: nobody likes writing boilerplate tests. Automating that frees up time for interesting work
- **Onboarding is faster**: a new developer can use skills from day 1 to understand the codebase

## Chapter 5: Resistance and Failures

### Human Resistance

Not everyone was enthusiastic at first:

**"It will replace us"** â€” The classic fear. I had to explain that Claude doesn't replace developers, it replaces tasks that developers don't enjoy doing. A senior spending 3h/day on code review isn't well utilized. A senior spending 3h/day on architecture design is.

**"Generated code is mediocre"** â€” True at first. The initial skills produced generic code. We had to iterate on prompts, add context (conventions, examples, existing patterns) to get usable output. It's a 2-3 week investment.

**"I prefer doing it myself"** â€” The "not invented here" syndrome applied to AI. Some developers took time to trust automated reviews. The key: showing that Claude finds bugs that humans miss.

### Failures

**"auto-fix-bug" skill** â€” We tried creating a skill that automatically fixes bugs from stacktraces. It worked for simple bugs (NPE, type mismatch) but failed on complex logic bugs. We transformed it into "analyze-bug" that proposes hypotheses rather than fixes.

**Initial overconfidence** â€” In the first weeks, some developers validated Claude's suggestions without verification. We had a minor incident (an E2E test that passed in CI but hid a false positive). It reminded us that AI is a tool, not an oracle.

**Token costs** â€” With 6 developers using Claude intensively, the monthly bill is significant. We had to optimize prompts and set usage limits to stay within budget.

## Chapter 6: Lessons Learned

### 1. Start Small, Iterate Fast

Don't launch 25 skills at once. We started with just one (review-pr), fine-tuned it for 2 weeks, then added others one by one. Each skill requires tuning specific to your team's context.

### 2. Context Is King

A generic prompt produces a generic result. Skill quality depends directly on the context you provide:
- Your team's naming conventions
- Examples of existing code
- Your project's architectural patterns
- Common errors to watch for

### 3. Keep Humans in the Loop

Claude doesn't replace human review, it prepares it. The optimal workflow: Claude does a first pass (conventions, tests, edge cases), the human reviewer focuses on business logic and architecture choices.

### 4. Measure, Measure, Measure

Without metrics, it's intuition. We set up a simple dashboard tracking time spent per task category. That's what allowed us to prove ROI and justify the budget.

### 5. Train the Team on Prompting

AI is only as good as the prompt you give it. We organized internal "prompt engineering" sessions so every developer could get the best out of Claude.

## Chapter 7: The Future â€” Where Are We Headed?

### What We're Preparing

- **Automatic review on every PR**: Claude triggers automatically on every GitLab merge request via webhook
- **Intelligent regression testing**: Claude identifies which tests should run based on modified files
- **Architecture assistant**: a skill that knows the ADR history and can suggest decisions consistent with the past

### My Conviction

We're at the very beginning of the AI revolution in engineering. In 2 years, not using AI in your development workflow will be as anachronistic as not using a linter.

Teams adopting these tools now will have a massive competitive advantage â€” not because AI is magic, but because it frees humans to do what they do best: think, design, innovate.

AI is not a gimmick. It's a **concrete force multiplier**. And the best time to integrate it is now.

---

*Chetana YIN â€” February 2026*
*Engineering Manager at DJUST, 25+ Claude Code skills in production.*`

const contentKm = `## áŸáŸá…á€áŸ’áá¸á•áŸ’áá¾á˜

á€áŸ’á“á»á„ááŸ‚áœá·á…áŸ’á†á·á€á¶ áŸ¢áŸ áŸ¢áŸ¤ ááŸ’á‰á»áŸ†á”á¶á“ášá€áƒá¾á‰ Claude CodeáŸ” á–á¸ášáŸá”áŸ’áá¶á áŸá€áŸ’ášáŸ„á™á˜á€ ááŸ’á‰á»áŸ†á”á¶á“ášá½á˜á”á‰áŸ’á…á¼á›áœá¶á€áŸ’á“á»á„ workflow á”áŸ’ášá…á¶áŸ†ááŸ’á„áŸƒášá”áŸáŸ‹á€áŸ’ášá»á˜áœá·áŸáŸ’áœá€áš áŸ¦ á“á¶á€áŸ‹ášá”áŸáŸ‹ááŸ’á‰á»áŸ†á“áŸ… DJUSTáŸ” á”á¸ááŸ‚á€áŸ’ášáŸ„á™á˜á€ á•á›á·áá—á¶á–ášá”áŸáŸ‹á™á¾á„á›á¾á€á·á…áŸ’á…á€á¶ášáŠáŠáŸ‚á›áŸ—á”á¶á“á€á¾á“á¡á¾á„ áŸ¤áŸ %áŸ”

á“áŸáŸ‡á˜á·á“á˜áŸ‚á“á‡á¶á¢ááŸ’áá”á‘á•áŸ’áŸá–áŸ’áœá•áŸ’áŸá¶á™á‘áŸáŸ” áœá¶á‡á¶ášá”á¶á™á€á¶ášááŸá”á‘á–á·áŸáŸ„á’á“áŸá–á·á â€” á‡á¶á˜á½á™á—á¶á–á‡áŸ„á‚á‡áŸá™ á€á¶ášá”ášá¶á‡áŸá™ á€á¶ášá”áŸ’ášá†á¶áŸ†á„ášá”áŸáŸ‹á˜á“á»áŸáŸ’áŸ á“á·á„á˜áŸášáŸ€á“áŠáŸ‚á›á”á¶á“ášáŸ€á“ â€” á¢áŸ†á–á¸á¢áŸ’áœá¸áŠáŸ‚á›áœá¶á˜á¶á“á“áŸá™á‡á¶á€áŸ‹áŸáŸ’ááŸ‚á„á€áŸ’á“á»á„á€á¶ášášá½á˜á”á‰áŸ’á…á¼á›á§á”á€ášááŸ AI á€áŸ’á“á»á„á€áŸ’ášá»á˜áœá·áŸáŸ’áœá€á˜áŸ’á˜áŠáŸ‚á›áŠáŸ†áá¾ášá€á¶ášá€áŸ’á“á»á„ productionáŸ”

---

## á‡áŸ†á–á¼á€á‘á¸ áŸ¡áŸ– á”ášá·á”á‘ â€” á€áŸ’ášá»á˜á€áŸ’ášáŸ„á˜áŸá˜áŸ’á–á¶á’

### DJUST á€áŸ’á“á»á„á†áŸ’á“á¶áŸ† áŸ¢áŸ áŸ¢áŸ¤

DJUST á‚áºá‡á¶áœáŸá‘á·á€á¶ e-commerce B2B SaaSáŸ” áœá·áŸá¶á›á—á¶á–ášá”áŸáŸ‹ááŸ’á‰á»áŸ†á€áŸ’á“á»á„á“á¶á˜á‡á¶ Engineering Manager á‚áŸ’ášá”áŠááŸ’áá”áŸ‹ Order Management System (OMS), Payments á“á·á„ CartáŸ” áœá¶á‡á¶áŸáŸ’á“á¼á›á”áŸ’ášáá·á”ááŸ’áá·á€á¶ášášá”áŸáŸ‹áœáŸá‘á·á€á¶ â€” á€á“áŸ’á›áŸ‚á„áŠáŸ‚á›á€á¶ášá”á‰áŸ’á‡á¶á‘á·á‰á á¼ášáŸá˜áŸ’ášá¶á”áŸ‹á¢áá·áá·á‡á“áŠá¼á…á‡á¶ Franprix, Eiffage (Blueon) á“á·á„ VEJAáŸ”

á€áŸ’ášá»á˜áŸ–
- áœá·áŸáŸ’áœá€áš áŸ¦ á“á¶á€áŸ‹ (áŸ¤ senior, áŸ¢ mid-level)
- StackáŸ– Java 17, Spring Boot, PostgreSQL, Elasticsearch, Kubernetes á›á¾ AWS
- ~áŸ¡áŸ¥ Maven modules áŠáŸ‚á›á–á¹á„á•áŸ’á¢áŸ‚á€á‚áŸ’á“á¶
- Releases ášáŸ€á„ášá¶á›áŸ‹ááŸ’á„áŸƒá–áŸ’ášá áŸáŸ’á”áá·áŸ
- SLA á€á·á…áŸ’á…áŸá“áŸ’á™á¶á‡á¶á˜á½á™á¢áá·áá·á‡á“ enterprise

### á”á‰áŸ’á á¶á•á›á·áá—á¶á–

áŠáŸ„á™áœá·á—á¶á‚á€á“áŸ’á›áŸ‚á„áŠáŸ‚á›á–áŸá›áœáŸá›á¶ášá”áŸáŸ‹á€áŸ’ášá»á˜ááŸ’ášá¼áœá”á¶á“á…áŸ†áá¶á™ ááŸ’á‰á»áŸ†á”á¶á“á€áŸ†áááŸ‹á‚áŸ†ášá¼áŠáŠáŸ‚á›áŸ—áŸ–

**áŸ£áŸ % á“áŸƒá–áŸá›áœáŸá›á¶ááŸ’ášá¼áœá”á¶á“á”áŸ’ášá¾á”áŸ’ášá¶áŸáŸ‹áŠáŸ„á™á€á·á…áŸ’á…á€á¶ášáŠáŠáŸ‚á›áŸ—áŠáŸ‚á›á˜á¶á“áá˜áŸ’á›áŸƒá‘á¶á”áŸ–**

- **Code reviews**áŸ– áŸ¢-áŸ£ á˜áŸ‰áŸ„á„á€áŸ’á“á»á„á˜á½á™ááŸ’á„áŸƒáŸá˜áŸ’ášá¶á”áŸ‹ááŸ’á‰á»áŸ†á€áŸ’á“á»á„á“á¶á˜á‡á¶ lead techniqueáŸ” PR á“á¸á˜á½á™áŸ—ááŸ’ášá¼áœá€á¶ášá€á¶ášá¢á¶á“áŠáŸ„á™á”áŸ’ášá»á„á”áŸ’ášá™áŸááŸ’á“ á˜áá·á™áŸ„á”á›áŸ‹á¢áŸ†á–á¸ style á€á¶ášá‚áŸ’ášá”áŠááŸ’áá”áŸ‹ tests edge cases
- **Tests boilerplate**áŸ– á€á¶ášáŸášáŸáŸáš unit tests áŸá˜áŸ’ášá¶á”áŸ‹ CRUDs, mappers, DTOs â€” á€á¼áŠáŠáŸ‚á›á¢á¶á…á–áŸ’á™á¶á€ášááŸá”á¶á“á”áŸ‰á»á“áŸ’ááŸ‚á…áŸ†áá¶á™á–áŸá›
- **Briefings deployment**áŸ– release á“á¸á˜á½á™áŸ—ááŸ’ášá¼áœá€á¶ášá¯á€áŸá¶ášáŸá„áŸ’ááŸá”á“áŸƒá€á¶ášá•áŸ’á›á¶áŸáŸ‹á”áŸ’áá¼áš á á¶á“á·á—áŸá™ rollback plans
- **á€á¶ášáœá·á—á¶á‚ bugs**áŸ– áŸáŸ’áœáŸ‚á„ášá€á€áŸ’á“á»á„ logs á•áŸ’á‚á¼á•áŸ’á‚á„ stacktraces á‡á¶á˜á½á™á€á¼áŠ á€áŸ†áááŸ‹ commit áŠáŸ‚á›á˜á¶á“á€áŸ†á á»áŸ
- **á¯á€áŸá¶áš**áŸ– á’áŸ’áœá¾á”á…áŸ’á…á»á”áŸ’á”á“áŸ’á“á—á¶á– ADRs, runbooks, READMEs á”á“áŸ’á‘á¶á”áŸ‹á–á¸á€á¶ášá•áŸ’á›á¶áŸáŸ‹á”áŸ’áá¼ášáŸáŸ’áá¶á”ááŸ’á™á€á˜áŸ’á˜á“á¸á˜á½á™áŸ—

á€á·á…áŸ’á…á€á¶ášá‘á¶áŸ†á„á“áŸáŸ‡á˜á·á“á˜áŸ‚á“á‚áŸ’á˜á¶á“á”áŸ’ášá™áŸ„á‡á“áŸá‘áŸ â€” áœá¶á…á¶áŸ†á”á¶á…áŸ‹áŸ” á”áŸ‰á»á“áŸ’ááŸ‚áœá¶ **á¢á¶á…á–áŸ’á™á¶á€ášááŸá”á¶á“ á“á·á„á˜á¶á“ášá…á“á¶áŸá˜áŸ’á–áŸá“áŸ’á’** áŠáŸ‚á›á’áŸ’áœá¾á±áŸ’á™áœá¶á›áŸ’á¢á¥áááŸ’á…áŸ„áŸ‡áŸá˜áŸ’ášá¶á”áŸ‹áŸáŸ’áœáŸá™á”áŸ’ášáœááŸ’áá·á€á˜áŸ’á˜áŠáŸ„á™ AIáŸ”

## á‡áŸ†á–á¼á€á‘á¸ áŸ¢áŸ– á€á¶ášášá€áƒá¾á‰ Claude Code

### á•áŸ’á›á¼áœá˜á»á“ Claude Code

á˜á»á“á–áŸá›á˜á€áŠá›áŸ‹ Claude Code ááŸ’á‰á»áŸ†á”á¶á“áŸáŸ’áœáŸ‚á„á™á›áŸ‹á§á”á€ášááŸ AI á•áŸ’áŸáŸá„á‘áŸ€ááŸ–

**GitHub Copilot** (áŸ¦ ááŸ‚) â€” Autocompletion á˜á¶á“á”áŸ’ášá™áŸ„á‡á“áŸá”áŸ‰á»á“áŸ’ááŸ‚á˜á¶á“á€á˜áŸ’ášá·ááŸ”
**Zencoder** â€” ááŸ’á‰á»áŸ†á”á¶á“á”áŸ’ášá¾ Zencoder áŠá¾á˜áŸ’á”á¸á‡á½á™á•áŸ’á‘áŸ€á„á•áŸ’á‘á¶ááŸ‹á€á·á…áŸ’á…á€á¶ášá˜á½á™á…áŸ†á“á½á“áŸ”
**Google Gemini** â€” ááŸ’á‰á»áŸ†á”á¶á“á”áŸ’ášá¾ Gemini á™áŸ‰á¶á„á…áŸ’ášá¾á“ášá™áŸˆá–áŸá›á‡á¶á…áŸ’ášá¾á“ááŸ‚áŸá˜áŸ’ášá¶á”áŸ‹á€á¶ášáŸáŸ’ášá¶áœá‡áŸ’ášá¶áœá”á…áŸ’á…áŸá€á‘áŸáŸáŸ”

### á áŸáá»á¢áŸ’áœá¸ Claude Code á”á¶á“á•áŸ’á›á¶áŸáŸ‹á”áŸ’áá¼ášá¢áŸ’áœá¸áŸ—á‘á¶áŸ†á„á¢áŸáŸ‹

á¢áŸ’áœá¸áŠáŸ‚á›á”á¶á“á”á‰áŸ’á…á»áŸ‡á”á‰áŸ’á…á¼á›ááŸ’á‰á»áŸ†á¢áŸ†á–á¸ Claude CodeáŸ–

1. **á€á¶ášá…á¼á›á”áŸ’ášá¾ codebase á–áŸá‰á›áŸá‰**áŸ– Claude Code áƒá¾á‰á¯á€áŸá¶ášá‚á˜áŸ’ášáŸ„á„á‘á¶áŸ†á„á¢áŸáŸ‹ á™á›áŸ‹áŸáŸ’áá¶á”ááŸ’á™á€á˜áŸ’á˜ conventions á“á·á„ patterns áŠáŸ‚á›á˜á¶á“áŸáŸ’ášá¶á”áŸ‹
2. **Skills á•áŸ’á‘á¶á›áŸ‹ááŸ’á›á½á“**áŸ– á¢áŸ’á“á€á¢á¶á…á”á„áŸ’á€á¾á prompts áŠáŸ‚á›á¢á¶á…á”áŸ’ášá¾á¡á¾á„áœá·á‰áŠáŸ‚á›ášá½á˜á”á‰áŸ’á…á¼á›á”ášá·á”á‘á¢á¶á‡á¸áœá€á˜áŸ’á˜
3. **á€á¶ášášá½á˜á”á‰áŸ’á…á¼á› MCP**áŸ– á€á¶ášáá—áŸ’á‡á¶á”áŸ‹áŠá¾á˜á‘áŸ… Slack, Jira, GitLab, Notion
4. **ášá”áŸ€á” agentic**áŸ– Claude á˜á·á“ááŸ’ášá¹á˜ááŸ‚áŸáŸ’á“á¾á€á¼áŠá‘áŸ áœá¶á¢á¶á…á”áŸ’ášáá·á”ááŸ’áá·á–á¶á€áŸ’á™á”á‰áŸ’á‡á¶ áŠáŸ†áá¾ášá€á¶áš tests á•áŸ’á‘áŸ€á„á•áŸ’á‘á¶ááŸ‹á€á¶áš compile

## á‡áŸ†á–á¼á€á‘á¸ áŸ£áŸ– á”áŸ’ášá–áŸá“áŸ’á’á¢áŸá€á¼á“áŸƒ 25+ Skills

á™á¾á„á”á¶á“ášáŸ€á”á…áŸ† skills ášá”áŸáŸ‹á™á¾á„á‡á¶ áŸ¥ á”áŸ’ášá—áŸá‘áŸ–

### 1. Code Quality (áŸ§ skills)
- **review-pr**áŸ– á€á¶ášáœá·á—á¶á‚ PR á–áŸá‰á›áŸá‰á‡á¶á˜á½á™á€á¶ášáŠá¶á€áŸ‹á–á·á“áŸ’á‘á»
- **review-security**áŸ– áŸáœá“á€á˜áŸ’á˜áŸá“áŸ’áá·áŸá»á (OWASP top 10)
- **review-perf**áŸ– á€á¶ášáœá·á—á¶á‚á”áŸ’ášáá·á”ááŸ’áá·á€á¶áš
- **check-conventions**áŸ– á€á¶ášá•áŸ’á‘áŸ€á„á•áŸ’á‘á¶ááŸ‹ conventions DJUST
- **suggest-refactor**áŸ– á€á¶ášáŸáŸ’á“á¾ refactoring á‡á¶á˜á½á™á€á¶ášá”á€áŸáŸ’ášá¶á™
- **check-api-contract**áŸ– á€á¶ášá•áŸ’á‘áŸ€á„á•áŸ’á‘á¶ááŸ‹á—á¶á–á†á”á‚áŸ’á“á¶áá™á€áŸ’ášáŸ„á™
- **check-migration**áŸ– á€á¶ášá•áŸ’á‘áŸ€á„á•áŸ’á‘á¶ááŸ‹ migrations DB

### 2. Testing (áŸ¥ skills)
### 3. Deployment & Ops (áŸ¥ skills)
### 4. Documentation (áŸ¤ skills)
### 5. Productivity (áŸ¤+ skills)

### MCPáŸ– Game-Changer á–á·á

á¢áŸ’áœá¸áŠáŸ‚á›á’áŸ’áœá¾á±áŸ’á™ skills á‘á¶áŸ†á„á“áŸáŸ‡á˜á¶á“áá¶á˜á–á›á–á·áá”áŸ’ášá¶á€áŠá‚áºá€á¶ášášá½á˜á”á‰áŸ’á…á¼á› MCP (Model Context Protocol)áŸ” Claude á—áŸ’á‡á¶á”áŸ‹áŠáŸ„á™á•áŸ’á‘á¶á›áŸ‹á‘áŸ…á§á”á€ášááŸášá”áŸáŸ‹á™á¾á„áŸ– GitLab, Jira, Slack, NotionáŸ”

## á‡áŸ†á–á¼á€á‘á¸ áŸ¤áŸ– á›á‘áŸ’á’á•á›áŠáŸ‚á›áœá¶áŸáŸ‹áœáŸ‚á„

| ášá„áŸ’áœá¶áŸáŸ‹ | á˜á»á“ | á€áŸ’ášáŸ„á™ | á—á¶á–áá»áŸá‚áŸ’á“á¶ |
|---------|------|-------|------------|
| á–áŸá›áœáŸá›á¶ code review á˜á’áŸ’á™á˜ | áŸ¤áŸ¥ á“á¶á‘á¸ | áŸ¡áŸ¥ á“á¶á‘á¸ | -áŸ¦áŸ§% |
| á–áŸá›áœáŸá›á¶áŸášáŸáŸáš tests | áŸ¢á˜áŸ‰áŸ„á„/feature | áŸ¤áŸ¥á“á¶á‘á¸/feature | -áŸ¦áŸ£% |
| á–áŸá›áœáŸá›á¶ briefing deployment | áŸ¡á˜áŸ‰áŸ„á„áŸ£áŸ  | áŸ¢áŸ  á“á¶á‘á¸ | -áŸ§áŸ¨% |
| Bugs ášá€áƒá¾á‰á€áŸ’á“á»á„ review | áŸ£.áŸ¢/PR | áŸ¥.áŸ¡/PR | +áŸ¥áŸ©% |
| Sprint velocity | áŸ¤áŸ¢ | áŸ¥áŸ¨ | +áŸ£áŸ¨% |

### á›áŸááŸáŸ†áá¶á“áŸ‹áŸ– +áŸ¤áŸ % á•á›á·áá—á¶á–

á›á¾ **á€á·á…áŸ’á…á€á¶ášáŠáŠáŸ‚á›áŸ—** á‡á¶á–á·áŸáŸáŸ á•á›á·áá—á¶á–á”á¶á“á€á¾á“á¡á¾á„ áŸ¤áŸ %áŸ”

## á‡áŸ†á–á¼á€á‘á¸ áŸ¥áŸ– á€á¶ášá”áŸ’ášá†á¶áŸ†á„ á“á·á„á€á¶ášá”ášá¶á‡áŸá™

**"áœá¶á“á¹á„á‡áŸ†á“á½áŸá™á¾á„"** â€” ááŸ’á‰á»áŸ†ááŸ’ášá¼áœá–á“áŸ’á™á›áŸ‹áá¶ Claude á˜á·á“á‡áŸ†á“á½áŸá¢áŸ’á“á€á¢á—á·áœáŒáŸ’áá“áŸá‘áŸ áœá¶á‡áŸ†á“á½áŸá€á·á…áŸ’á…á€á¶ášáŠáŸ‚á›á¢áŸ’á“á€á¢á—á·áœáŒáŸ’áá“áŸá˜á·á“á…á¼á›á…á·ááŸ’áá’áŸ’áœá¾áŸ”

**"á€á¼áŠáŠáŸ‚á›á”á„áŸ’á€á¾áá˜á¶á“á‚á»áá—á¶á–á‘á¶á”"** â€” á–á·áá“áŸ…áŠáŸ†á”á¼á„áŸ” á™á¾á„ááŸ’ášá¼áœá’áŸ’áœá¾á¡á¾á„áœá·á‰á›á¾ prompts á”á“áŸ’ááŸ‚á˜á”ášá·á”á‘ áŠá¾á˜áŸ’á”á¸á‘á‘á½á›á”á¶á“ output áŠáŸ‚á›á¢á¶á…á”áŸ’ášá¾á”á¶á“áŸ”

## á‡áŸ†á–á¼á€á‘á¸ áŸ¦áŸ– á˜áŸášáŸ€á“áŠáŸ‚á›á”á¶á“ášáŸ€á“

1. **á…á¶á”áŸ‹á•áŸ’áá¾á˜áá¼á… á’áŸ’áœá¾á¡á¾á„áœá·á‰á›á¿á“** â€” á€á»áŸ†á…á¶á”áŸ‹á•áŸ’áá¾á˜ 25 skills á€áŸ’á“á»á„á–áŸá›ááŸ‚á˜á½á™
2. **á”ášá·á”á‘á‡á¶áŸáŸ’ááŸá…** â€” Prompt á‘á¼á‘áŸ…á•á›á·áá›á‘áŸ’á’á•á›á‘á¼á‘áŸ…
3. **ášá€áŸ’áŸá¶á˜á“á»áŸáŸ’áŸá€áŸ’á“á»á„ášá„áŸ’áœá„áŸ‹** â€” Claude ášáŸ€á”á…áŸ† review á˜á“á»áŸáŸ’áŸá•áŸ’ááŸ„áá›á¾áá€áŸ’á€áœá·á‡áŸ’á‡á¶á¢á¶á‡á¸áœá€á˜áŸ’á˜
4. **áœá¶áŸáŸ‹ áœá¶áŸáŸ‹ áœá¶áŸáŸ‹** â€” á‚áŸ’á˜á¶á“ metrics áœá¶á‡á¶á€á¶ášáŸáŸ’á˜á¶á“
5. **á”ááŸ’áá»áŸ‡á”ááŸ’áá¶á›á€áŸ’ášá»á˜á›á¾ prompting** â€” AI á›áŸ’á¢á”áŸ‰á»ááŸ’áá¶á€áŸáŠáŸ„á™ prompt áŠáŸ‚á›á¢áŸ’á“á€á•áŸ’áá›áŸ‹

## á‡áŸ†á–á¼á€á‘á¸ áŸ§áŸ– á¢á“á¶á‚á

á™á¾á„áŸáŸ’áá·áá“áŸ…áŠá¾á˜áŠáŸ†á”á¼á„á“áŸƒá”áŠá·áœááŸ’áá“áŸ AI á€áŸ’á“á»á„áœá·áŸáŸ’áœá€á˜áŸ’á˜áŸ” á€áŸ’á“á»á„ášá™áŸˆá–áŸá› áŸ¢ á†áŸ’á“á¶áŸ† á€á¶ášá˜á·á“á”áŸ’ášá¾ AI á€áŸ’á“á»á„ workflow á¢á—á·áœáŒáŸ’áá“áŸášá”áŸáŸ‹á¢áŸ’á“á€á“á¹á„á á½org áŸá˜áŸá™áŠá¼á…á‡á¶á€á¶ášá˜á·á“á”áŸ’ášá¾ linteráŸ”

AI á˜á·á“á˜áŸ‚á“á‡á¶ášá¿á„á›áŸá„á‘áŸáŸ” áœá¶á‡á¶ **á€á˜áŸ’á›á¶áŸ†á„á–á„áŸ’ášá¸á€á‡á¶á€áŸ‹áŸáŸ’ááŸ‚á„**áŸ” á á¾á™á–áŸá›áœáŸá›á¶á›áŸ’á¢á”áŸ†á•á»ááŠá¾á˜áŸ’á”á¸ášá½á˜á”á‰áŸ’á…á¼á›áœá¶á‚áºá¥á¡á¼áœá“áŸáŸ‡áŸ”

---

*Chetana YIN â€” á€á»á˜áŸ’á—áŸˆ áŸ¢áŸ áŸ¢áŸ¦*
*Engineering Manager á“áŸ… DJUST, 25+ Claude Code skills á€áŸ’á“á»á„ productionáŸ”*`

async function seedBlogClaudeCode() {
  console.log('ğŸ¤– Seeding blog article: Claude Code in Engineering Team...')

  await db.delete(blogPosts).where(eq(blogPosts.slug, 'claude-code-equipe-engineering'))

  await db.insert(blogPosts).values({
    slug: 'claude-code-equipe-engineering',
    titleFr: "Comment j'ai intÃ©grÃ© Claude Code dans mon Ã©quipe d'engineering",
    titleEn: "How I integrated Claude Code into my engineering team",
    titleKm: "ášá”áŸ€á”áŠáŸ‚á›ááŸ’á‰á»áŸ†á”á¶á“ášá½á˜á”á‰áŸ’á…á¼á› Claude Code á€áŸ’á“á»á„á€áŸ’ášá»á˜áœá·áŸáŸ’áœá€á˜áŸ’á˜ášá”áŸáŸ‹ááŸ’á‰á»áŸ†",
    contentFr,
    contentEn,
    contentKm,
    excerptFr: "Retour d'expÃ©rience sur l'intÃ©gration de Claude Code dans une Ã©quipe de 6 ingÃ©nieurs : 25+ skills personnalisÃ©s, +40% de productivitÃ©, rÃ©sistances humaines et leÃ§ons apprises.",
    excerptEn: "Experience report on integrating Claude Code into a team of 6 engineers: 25+ custom skills, +40% productivity, human resistance and lessons learned.",
    excerptKm: "ášá”á¶á™á€á¶ášááŸá”á‘á–á·áŸáŸ„á’á“áŸá›á¾á€á¶ášášá½á˜á”á‰áŸ’á…á¼á› Claude Code á€áŸ’á“á»á„á€áŸ’ášá»á˜áœá·áŸáŸ’áœá€áš áŸ¦ á“á¶á€áŸ‹áŸ– 25+ skills á•áŸ’á‘á¶á›áŸ‹ááŸ’á›á½á“ +áŸ¤áŸ % á•á›á·áá—á¶á– á€á¶ášá”áŸ’ášá†á¶áŸ†á„ á“á·á„á˜áŸášáŸ€á“áŠáŸ‚á›á”á¶á“ášáŸ€á“áŸ”",
    tags: ['AI', 'Claude Code', 'Management', 'Productivity', 'MCP'],
    published: true
  })

  console.log('âœ… Blog article seeded successfully!')
}

seedBlogClaudeCode().catch(console.error)
